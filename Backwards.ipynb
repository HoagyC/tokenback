{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nhIKn2YkJ2r","executionInfo":{"status":"ok","timestamp":1673517478535,"user_tz":0,"elapsed":29353,"user":{"displayName":"Jessica Rumbelow","userId":"14021829986331042199"}},"outputId":"755135fc-6e03-4f71-a4e3-f9db14bcc573"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ar1aNn7TxI8r"},"outputs":[],"source":["!pip install update transformers\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, utils\n","import torch\n","import random\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","from IPython import display\n","import numpy as np\n","from tqdm import tqdm\n","from time import time\n","import json\n","utils.logging.set_verbosity_error()\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","vocab_len= 50257\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left')\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id, vocab_size=vocab_len).to(device)\n","model.eval()\n","# the model will be in evaluation, not training, mode throughout\n","word_embeddings = model.transformer.wte.weight.to(device)  \n","embedding_dim = word_embeddings.shape[-1] \n","# 'word_embeddings' tensor gives emeddings for each token in the vocab for this model,\n","# has shape (vocab_len, embedding_dimension) which in this case = (50257, 768)\n"]},{"cell_type":"code","source":["common_centroid_nearest_token_idxs = [30212, 187, 195, 216, 182, 179, 213, 39820, 199, 124,  208, 125, 23090, 554, 30208, 281,  3607,  7003, 192, 37528,  15524, 217, 39752, 42089, 183, 818, 210, 201, 209, 207, 211, 206,  1026, 189, 190,  1315, 219, 205, 212, 203,  287, 188, 30898, 45544, 14827, 218, 30897, 202, 181, 30905]\n","# Having run 500 batches of 50 and used kmeans, these were the 50 most common closest token embeddings to centroids that were found (from most to least common).\n"],"metadata":{"id":"zFzbf-b0LC2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCgMUuO_33Wp"},"outputs":[],"source":["def normalise(x, min_max=[]):     \n","# normalises values of (array or tensor) x according to first (min) and second (max) values in list min_max. \n","# This effectively defaults to [0,1] if the list doesn't contain exactly two elements. \n","# The original code threw an error if min_max had length 1, so it's been changed slightly.\n","\n","# First normalise x to [0,1]\n","    rnge = x.max() - x.min()\n","    if rnge > 0:\n","        x = (x - x.min())/rnge\n","\n","# Now, if there's a min and max given in min_max list, multiply by difference and add minimum\n","    if len(min_max) > 1:\n","        rnge = min_max[1] - min_max[0]\n","        x = x * rnge + min_max[0]\n","\n","    return x\n","\n","\n","def closest_tokens(emb, n=1):      \n","# This finds the n tokens in the vocabulary that are closest in the embedding space (in terms of Euclidean distance) to a given word embedding (‘emb’).\n","# Note that here 'emb' may or may not correspond to a token (i.e., it may or may not be a 'legal' embedding).\n","# Function returns a 4-tuple (list of the n tokens, list of their indices, list of their distances from emb, and list of their embedding vectors)\n","    torch.cuda.empty_cache()\n","    dists = torch.linalg.norm(word_embeddings - emb, dim=1)\n","    sorted_dists, ix = torch.sort(dists)\t \n","    # sorted_dists is a list of all embedding distances from 'emb', across entire vocab, sorted in increasing order, \n","    # ix is a list of their corresponding 'vocab indices'\n","    tokens = [tokenizer.decode(i) for i in ix[:n]]\n","    # For each of the first n 'vocab indices' in ix, we decode it into the string version of the corresponding token. \n","    # These strings then constitute the list 'tokens'.\n","    ixs = ix[:n]\n","    dists = sorted_dists[:n]\n","    embs = word_embeddings[ixs]  # Each of these n 'embeddings' is a tensor of shape (768,)\n","    return tokens, ixs, dists, embs  \n","\n","\n","def model_emb(inputs_embeds, output_len):\n","# 'input_embeds' is a tensor of shape (batch_size, input_len, embedding_dim)\n","# 'output_len' is an integer specifying the number of output tokens to generate\n","# Note that this function doesn't involve a target output. It simply takes a tensor of input embeddings (based on input length),\n","# calculates perplexities for that batch of input sequences,\n","# and runs the batch of input sequences through GPT2, for each finding next tokens iteratively 'output_len' number of times\n","    embs = inputs_embeds   # This is going to get expanded using 'output_embs'\n","    logits = []\n","    ixs = []\n","    input_logits = None\n","    for i in range(output_len):\n","        model_out = model(inputs_embeds=embs, return_dict=True)\n","        # Does a forward pass of GPT2 (or whichever model) on a batch of inputs (given as a tensor 'embs' of embeddings).\n","        # This 'embs' will expand along its 1st dimension with each iteration.\n","        # Outputs logits and more (hidden states, attention, etc.) as a dictionary 'model_out'.\n","        # But we'll only be concerned with model_out.logits.\n","\n","        if i == 0:\n","            input_logits = model_out.logits \n","            # On first pass through loop, we simply use the logits of the model output\n","            # That's a tensor of shape (batch_size, input_len, vocab_size) giving logits for each input in each batch.\n","            # Presumably for each input, this is conditioned on the inputs that preceded it?\n","\n","        # On every pass throught the loop (including the first), we defined this tensor of shape (batch_size, 1, vocab_size):\n","        last_logits = model_out.logits[:,-1].unsqueeze(1)  \n","        # model_out.logits[:,-1] will be a 2D tensor of shape (batch_size, vocab_size), just giving logits for last input/embedding across all batches/tokens\n","        # unsqueezing, we get tensor of shape (batch_size, 1, vocab_size) also giving logits of last input/embedding, differently formatted  \n","        logits.append(last_logits)  # appends last_logits tensor to the 'logits' list \n","        ix = torch.argmax(last_logits, dim=-1)  # for each batch, finds the vocab index of the token with the largest logit in last_logits\n","        ixs.append(ix) # ...and appends this tensor of shape (batch_size,) (containing indices) it to the list 'ixs'\n","        output_embs = word_embeddings[ix]   # for each batch, finds embedding for the token with that index...\n","        embs = torch.cat([embs, output_embs], dim=1)  #...concatenates that tensor of embeddings to the 'embs' tensor in the first dimension before next iteration\n","\n","     # When the loop is completed 'embs' will be a tensor containing all of the input and output word embeddings produced by the model   \n","     # ...so presumably of shape (batch_size, input_len + output_len, embedding_dim)\n","\n","    logits = torch.cat(logits, dim=1)   # this converts logits from a list of tensors to a single tensor, by concatenating all of the tensors in the list\n","                                        # it will have shape (batch_size, output_len, vocab_size)\n","    perp = perplexity(input_logits)     # 'input_logits' was calculated on first pass through loop where only input embeddings were involved\n","    return logits, embs, perp          \n","    # logits has shape (batch_size, output_len, vocab_size),         CHECK THAT!\n","    # embs has shape (batch_size, input_len + output_len, embedding_dim)\n","    # perp has shape (batch_size,)\n","\n","\n","def perplexity(logits):\n","    # logits is of shape (batch_size, 'sequence length', vocab_size)\n","    # for all current calls, 'sequence length' is going to be input_len\n","    probs, ix = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n","    # torch.softmax(logits, dim=-1) will also be a tensor of shape (batch_size, 'sequence length', vocab_size), \n","    # but where the logits in the last dimension get converted into probabilities via softmax. torch.max() then pull out the largest of these and its index\n","    # probs is a tensor that contains the maximum probability for each token in the embedding sequence, shape (batch_size, 'sequence length')\n","    # ix is a tensor that contains the corresponding indices, also with shape (batch_size, 'sequence length')\n","    perp = 1/ (torch.prod(probs, dim=-1)**(1/probs.shape[-1])) - 1\n","    # defines a scalar that's larger with greater uncertainty (so if the probs are small, their product is small, the reciprocal of some power is large)\n","    # probs.shape[-1] is output_len; the idea of raising the probs product to power 1/output_len is to make perplexities comparable across different output lengths\n","    return perp\n","\n","\n","# Here's the key function that optimises for a sequence of input embeddings, given a target_output string:\n","def optimise_input(epochs=100, \n","                   lr=0.1, \n","                   rand_after=False,    # Do we re-initialise inputs tensor with random entries when an optimal input is found?\n","                   w_freq=10,           # logging (write) frequency\n","                   base_input=False,      # If False, start_inputs will be entirely random\n","                   batch_size=1, \n","                   input_len=1, \n","                   target_output=tokenizer.eos_token,    # Default target output is the \"end-of-string\" token; this won't generally be used\n","                   output_len=None,\n","                   dist_reg=1,       # distance regularisation coefficient\n","                   perp_reg=0,       # perplexity regularisation coefficient; setting to 0 means perplexity loss isn't a thing\n","                   plt_loss=False,   # Do we plot loss?\n","                   loss_type='log_prob_loss', \n","                   seed=0,\n","                   return_early=True,    # finishes if single optimised input is found\n","                   verbose=0,            # Controls how much info gets logged.\n","                   lr_decay=False,       # Use learning rate decay? If so, a scheduler gets invoked.\n","                   noise_coeff = 0.01, **kwargs):     # Introduced for generality in the construction of start_input[1:] below.\n","    \n","    torch.manual_seed(seed)               # sets up PyTorch random number generator\n","\n","    results = {'args': locals()}          # locals() is a built-in Python function that returns a dictionary of the local symbol table. \n","                                          # In this case, it is returning a dictionary of all the local variables in the current scope and \n","                                          # storing them in the results variable.\n","\n","    if plt_loss:\n","        plt.rcParams.update({'figure.figsize': (40,6)})\n","\n","    total_losses = []\n","    losses = []\n","    dists = []\n","    perps = []\n","    optimised_inputs = dict()\n","    done = None\n","\n","    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n","    # output_ix is a 1-D tensor of shape (output_len,) that contains the indices of the tokens in the encoding of the string 'target_output'\n","    # tokenizer.encode(target_output, return_tensors='pt') is a list containing this one tensor, hence the need for the [0]\n","    # \"return_tensors='pt'\" ensures that we get a tensor in PyTorch format\n","\n","    if output_len == None or output_len < output_ix.shape[0]:  # This won't generally be the case, but if we don't specify output_len (i.e. it's == None), then...\n","        output_len = output_ix.shape[0]       # ...it will be set to the number of tokens in the encoding of the string 'target_output'\n","    # There will be situations where we want output_len to be of a different size to the number of tokens in target_output?\n","\n","    print('Optimising input of length {} to maximise output logits for \"{}\"'.format(input_len, target_output))\n","    # Typically this would print something like 'Optimising input of length 6 to maximise output logits for \"KILL ALL HUMANS!\"'.\n","\n","    if base_input == False:\n","        start_input = torch.rand(batch_size, input_len, embedding_dim).to(device)\n","        # If no base_input is provided, we construct start_input as a random tensor \n","        # of shape (batch_size, input_len, embedding_dim) (embedding_dim = 768 for this GPT-2 model).\n","        start_input = normalise(start_input,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n","        # We normalise this random tensor so that its minimum and maximum values correspond to those in the entire word_embeddings tensor\n","        # This dispenses with whole swathes of \"input space\" which contain no legal token embeddings \n","        # (we're limiting ourselves to a kind of \"hull\" defined by the 50527 vocab tokens in the embedding space), \n","        # which is a sensible place to look for optimised inputs.\n","    else:\n","        start_input = torch.zeros((batch_size, input_len, embedding_dim))\n","        # The following randomly samples embeddings from the list of 50 most common tokens whose embeddings \n","        # are closest to centroid embeddings after running 500 batches of 50 in kmeans.\n","        for i in range(batch_size):\n","            random_indices = random.sample(range(50), input_len)\n","            for j, index in enumerate(random_indices):\n","                start_input[i, j, :] = word_embeddings[common_centroid_nearest_token_idxs[index], :]\n","\n","        # The code and comments following \"else:\" temporarily replaces the following lines:\n","        #start_input = word_embeddings[output_ix].mean(dim=0).repeat(batch_size, input_len, 1)\n","        # Instead of a tensor of random embeddings, we use the mean embedding of all the tokens in the target output string and stack into a tensor. \n","        #if batch_size > 1:\n","        #    start_input[1:] += (torch.rand_like(start_input[1:]) + torch.full_like(start_input[1:], -0.5)) * noise_coeff\n","        #...and if we have more than one element in our batch, we \"noise\" the rest. \n","        # This was originally done using \"*=\" (multiplying entries by small random numbers)\n","        # We've changed this to \"+=\" (adding  small random numbers instead of multiplying by them).\n","        # The original code would have pushed everything in a positive direction, hence the use of a tensor full of -0.5's.       \n","\n","    \n","    input = torch.nn.Parameter(start_input, requires_grad=True)\n","    # input is not a tensor, it's a Parameter object that wraps a tensor and adds additional functionality. \n","    # 'input.data' is used below\n","    \n","    optimiser = torch.optim.Adam([input], lr=lr)\n","    # standard optimiser; note that it generally operates on a list of tensors, so we're giving it a list of one tensor; standard learning rate\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5)\n","    # this is used when loss hasn't improved for 20 timesteps; this scheduler will reduce the lr by a 'factor' of 0.5 when the \n","    # validation loss stops improving for 'patience' (here 20) epochs, and will wait 'cooldown' (here 20) epochs before resuming normal operation.\n","\n","    # now we loop across training epochs\n","    for e in range(epochs):\n","        torch.cuda.empty_cache()\n","        input = input.to('cuda')\n","        logits, emb, perp = model_emb(torch.clamp(input, word_embeddings.min(), word_embeddings.max()), output_len)\n","        # Does forward pass on a 'clamped' version of the 'input' tensor (done to contain it within the 'hull' of the vocabulary within 'input space').\n","        # Iterates to produce an output of output_len tokens, \n","        # returns: 'logits' = tensor of logits for output, of shape (batch_size, output_len, vocab_size)\n","        # 'emb': tensor of embeddings for input+output of shape (batch_size, input_len + output_len, embedding_dim); \n","        # 'perp': the input sequence perplexities tensor, of shape (batch_size,)\n","        probs = torch.softmax(logits, dim=-1)\n","        # For each batch, output, converts the sequence of logits (of length 'vocab_size') in the 'logits' tensor to probabilities, using softmax\n","\n","        logits = (logits - logits.min(dim=-1)[0].unsqueeze(-1)) / (logits.max(dim=-1)[0].unsqueeze(-1) - logits.min(dim=-1)[0].unsqueeze(-1))\n","        # This appears to be normalising the logits for each batch/output embedding so they're all between 0 and 1... \n","        # This is for ease of visualisation.\n","\n","        perp_loss = perp.mean() * perp_reg\n","        # That's taking the mean perp value across all batches, then regularising it. Currently perp_reg is set to 0, so perp_loss = 0.\n","\n","        if output_len > output_ix.shape[0]:\n","            target_logits = torch.stack([logits[:, :, ix] for ix in output_ix], dim=-1)\n","            target_logits = torch.max(target_logits, dim=-1)[0]\n","            # logits is shape (batch_size, output_len, vocab_size) \n","            # We throw out everything in the final dimension except those logits corresponding to indices of tokens in the target_ouput\n","            # This gives tensor with shape (batch_size, output_len, output_ix.shape[0])\n","            # We then take the maximum of those for each batch, output; this gives shape (batch_size, output_len)\n","            # The [0] returns just the max (torch.max returns (max, indices) tuple)\n","            target_probs = torch.stack([probs[:, :, ix] for ix in output_ix], dim=-1)\n","            target_probs = torch.max(target_probs, dim=-1)[0]\n","            # This does the analogous thing for probs.\n","\n","        else:\n","            target_logits = torch.stack([logits[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n","            target_probs = torch.stack([probs[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n","            # This handles case where output_len == output_ix.shape[0]\n","            # target_logits now of shape (batch_size, output_len)?\n","            # output_len < output_ix.shape[0] was dealt with in line 135\n","            \n","        token_dist = torch.stack([torch.stack([closest_tokens(e)[2].squeeze(-1) for e in input[b]]) for b in range(batch_size)])\n","        # As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\n","        # legal token embedding for each input embedding in each batch\n","        mean_token_dist = token_dist.mean() * dist_reg\n","        # A single scalar value, taking mean across the batch and input embeddings? \n","\n","\n","        # There are currently four loss types, many more could be introduced.\n","        # log_prob_loss is the current default.\n","        if loss_type == 'logit_loss':\n","            loss = 1-target_logits\n","        elif loss_type == 'log_prob_loss':\n","            loss = -torch.log(target_probs)\n","        elif loss_type == 'prob_loss':\n","            loss = 1-target_probs\n","        elif loss_type == 'CE':\n","            loss = torch.nn.functional.cross_entropy(logits.swapaxes(-1,-2), output_ix.repeat(batch_size, 1), reduction=None)\n","        else:\n","            print(loss_type + 'is not implemented.')\n","            return \n","\n","        batch_loss = loss.mean()\n","\n","        total_loss = torch.stack([mean_token_dist, batch_loss, perp_loss]).mean()\n","        # This is this just (mean_token_dist + loss + perp_loss)/3 tensorised across batches, yes?\n","\n","        total_losses.append(total_loss.detach().cpu().data)\n","        losses.append(batch_loss.detach().cpu().data)\n","        dists.append(mean_token_dist.detach().cpu().data)\n","        perps.append(perp_loss.detach().cpu().data)\n","        # these four lists were intialised above. We're appeneding to the list each epoch. All are scalars.\n","\n","        closest_ix = torch.stack([torch.stack([closest_tokens(e)[1] for e in b]) for b in input]).squeeze(-1)\n","        # This is similar to above, but building a tensor of indices of nearest embeddings, rather than distances.\n","        # Iterates over batches, and for each batch iterates over embeddings, giving tensor of shape (batch_size, input_len).\n","\n","        model_outs = model.generate(closest_ix, max_length = output_len+input_len)\n","        # The 'closest_ix' tensor is passed as the initial input sequence to the model, \n","        # and the max_length parameter specifies the maximum length of the total sequence to generate.\n","        # The output sequence will be terminated either when the end-of-sequence token is generated \n","        # or when the maximum length is reached, whichever occurs first.\n","        # \n","        # The output of the model.generate method will be a tuple containing the generated sequences and the model's internal states. \n","        # The generated sequences will be stored in a tensor of shape (batch_size, output_len+input_len). \n","        # Each element of the tensor will be a sequence of tokens with a length of at most output_len+input_len.\n","        \n","        for b in range(batch_size):\n","\n","            if output_len > output_ix.shape[0]:\n","                if target_output in tokenizer.decode(model_outs[b][input_len:]):\n","                    done = tokenizer.decode(model_outs[b][:input_len])\n","                    optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n","                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n","                # we decode these as tokens... is the target_output a substring?\n","                # if so, we print the target_output and the decoded string that contains it\n","                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n","\n","            if tokenizer.decode(model_outs[b][input_len:]) == target_output:\n","                done = tokenizer.decode(model_outs[b][:input_len])\n","                optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n","                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n","                # we decode these as tokens... is the target_output equal to output string?\n","                # Nothing printed in this case.\n","                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n","            \n","            if done is not None and rand_after:\n","                input.data[b] = torch.rand_like(input[b])\n","                # Random re-initialisation (if 'rand_after' set to True)\n","\n","  \n","        if ((e+1) % w_freq == 0) or done and return_early:\n","            display.clear_output(wait=True) \n","            print('\\033[H\\033[J')  \n","        # Every w epochs we write to log, unless we have found an optimised input before that and 'return_early' == True. \n","        # I'm still not entirely sure about the idea of 'return_early'.\n","\n","            if plt_loss:\n","                plt.plot(range(len(total_losses)), total_losses, label='Total Loss', color='black')\n","                plt.plot(range(len(losses)), losses, label='Output Loss')\n","                plt.plot(range(len(dists)), dists, label='Emb Dist Loss')\n","                plt.plot(range(len(perps)), perps, label='Perp Loss')\n","                plt.yscale('log')\n","                plt.legend()\n","\n","                plt.show()\n","\n","            print('Inputs found: ', optimised_inputs)\n","            print('{}/{} Output Loss: {} Emb Dist Loss: {} Perp Loss: {} LR: {}'.format(e+1, epochs, batch_loss, mean_token_dist, perp_loss, optimiser.param_groups[0]['lr']))\n","            if verbose == 3:\n","                print('Target Probs: {}\\nTarget Logits: {}\\nInput Dists: {}\\nInput Perplexity: {}\\n'.format(target_probs.detach().cpu().numpy(), target_logits.detach().cpu().numpy(), token_dist.detach().cpu().numpy(), perp.detach().reshape(-1).cpu().numpy()))\n","            # Optimised inputs and additional information are printed as part of log\n","\n","            for b in range(batch_size):\n","                if verbose > 0:\n","                    if verbose == 2:\n","                        print(b, repr(' Raw embeddings: {}'.format(''.join([closest_tokens(e)[0][0] for e in emb[b]]))))\n","                        # Change name to clarify (output of model if we just put in raw embeddings)\n","                        # prints batch number; closest_tokens(e)[0] is a list of tokens, closest_tokens(e)[0] is the first (closest) of these\n","                        # these get joined with separator '' (SHOULDN'T THAT BE ' '?)  \n","                    print(b, repr(' Closest embeddings: {}'.format(tokenizer.decode(model_outs[b]), '\\n')))\n","                        # WON'T THIS give string decodings of the embeddings, rather than the embeddings themselves?\n","                else:\n","                    print(repr(tokenizer.decode(model_outs[b])), end=' ')\n","                    # The least verbose printed output. The 'end' parameter is used to specify the end-of-line string that is appended to the output. \n","                    # By default, this is a newline character, but in this case it has been set to a single space character, \n","                    # so the output will be separated by spaces rather than newlines.\n","\n","            if done and return_early:\n","                print('\\nOptimised Input: \"{}\"'.format(done))\n","                results['optimised_inputs'] = optimised_inputs\n","                return results\n","                # we know optimised_inputs set contains a single element in this case\n","            \n","        optimiser.zero_grad()\n","        total_loss.backward()\n","        optimiser.step()\n","        # I assume these three lines are standard NN optimisation stuff?\n","\n","        if lr_decay:\n","            scheduler.step(total_loss)\n","         # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5) gets used if lr_decay == True\n","        done = None\n","\n","    results['optimised_inputs'] = optimised_inputs\n","    return results\n","    # that's a set of strings\n","\n"]},{"cell_type":"code","source":["# attempt to implement k-means algorithm\n","\n","def kmeans(num_clusters):\n","\n","    empty_cluster = True\n","\n","    # euclidean distance threshold to break out of loop\n","    threshold = 0.01\n","\n","    while empty_cluster == True: \n","        # randomly generate num_clusters centroids, stack these into a tensor of shape (num_clusters, 768)\n","        # then normalise to vocab embedding span\n","        centroids = torch.rand(num_clusters, embedding_dim).to(device)\n","        centroids = normalise(centroids,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n","\n","        distances = torch.cdist(word_embeddings, centroids, p=2)\n","        # This will be of shape (vocab_len, num_clusters), recording distances of token embeddings from each of the centroid.\n","        closest_distance, closest_centroid = torch.min(distances, dim = -1)\n","        # These will be of shape (vocab_len,), recording the distance of each token embedding to nearest centroid, and the index of that centroid.\n","\n","        clusters = []\n","        mean_distances = []\n","        # We iterate over the centroids\n","        for i in range(centroids.shape[0]):\n","            mask = closest_centroid == i  # This builds a Boolean mask over the complete set of tokens, True if a token's nearest centroid is the ith.\n","            clusters.append(word_embeddings[mask]) \n","            # word_embeddings[mask] is a subtensor of the (50257, 768) shape tensor involving only tokens nearest ith centroid.\n","            # This subtensor gets appended to the list 'clusters', which has num_clusters elements.\n","\n","        shapes = [clusters[i].shape[0] for i in range(len(clusters))]  \n","        if 0 not in shapes:\n","            empty_cluster = False \n","\n","    # Now we have a set on num_clusters non-empty clusters, we begin iterating centroid positions:\n","    centroids_stable = False\n","    iterations = 0\n","\n","    while centroids_stable == False:\n","        iterations += 1\n","        distances = torch.cdist(word_embeddings, centroids, p=2)\n","        # This will be of shape (vocab_len, num_clusters), recording distances of token embeddings from each of the centroid.\n","        closest_distance, closest_centroid = torch.min(distances, dim = -1)\n","        # These will be of shape (vocab_len,), recording the distance of each token embedding to nearest centroid, and the index of that centroid.\n","\n","        clusters = []\n","        mean_distances = []\n","        # We iterate over the centroids\n","        for i in range(centroids.shape[0]):\n","            mask = closest_centroid == i  # This builds a Boolean mask over the complete set of tokens, True if a token's nearest centroid is the ith.\n","            clusters.append(word_embeddings[mask]) \n","            # word_embeddings[mask] is a subtensor of the (50257, 768) shape tensor involving only tokens nearest ith centroid.\n","            # This subtensor gets appended to the list 'clusters' (num_clusters elements).\n","            mean_distances.append(closest_distance[mask].mean().item()) \n","            # closest_distance[mask] is a subtensor of the (50257,) shape tensor involving only tokens nearest ith centroid.\n","            # This subtensor gets appended to the list 'mean_distances' (num_clusters elements).\n","\n","        new_centroids = []\n","        for i in range(num_clusters):\n","            new_centroids.append(clusters[i].mean(dim=0))\n","            # clusters[i].mean(dim=0) is the centroid of the set of vectors encoded in the tensor clusters[i]\n","            # these all get put in a list...\n","\n","        new_centroids = torch.stack(new_centroids)\n","        # ... and the list gets stacked into a tensor of shape (num_clusters, 768)\n","\n","        # We now compute the euclidean distance between old and new centroids\n","        distance = torch.norm(new_centroids - centroids, dim=-1)\n","        print('iteration: ', iterations, '\\n max distance between old and new centroids:', torch.max(distance).item())\n","        # This is returning a \"nan\" for all iterations in some cases, usually when num_clusters > 10\n","        # I can't figure out why this is happening, but possibly some error handling could just take us back \n","        # to the random initialisation of the 'centroids' tensor.\n","\n","        if torch.max(distance) < threshold:\n","            centroids_stable == True\n","            break\n","        centroids = new_centroids # if we're still outside the distance threshold, keep iterating\n","    \n"," \n","    for i in range(len(clusters)):\n","        print('Cluster', i, ' contains ', len(clusters[i]), ' embeddings.')\n","    for j in range(num_clusters):\n","        print('Closest token to centroid ', j,': ', closest_tokens(centroids[j])[0], closest_tokens(centroids[j])[1].item())\n","    return centroids"],"metadata":{"id":"2jZJMakszJ_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tev0cDbkdbxO"},"outputs":[],"source":["ix = tokenizer.encode(\"\")\n","# list of 'vocab indices'\n","print(ix)\n","print([tokenizer.decode(i) for i in ix])\n","# prints reconstruction of input string\n","print(len(ix))\n","# prints number of tokens\n","output_len=2\n","model_out = model.generate(torch.tensor(ix).unsqueeze(0).to(device), max_length = output_len + len(ix))\n","print(tokenizer.decode(model_out[0]))\n","# pushes input string throught GPT2 (or whichever model) iteratively producing output_len number of tokens, then prints input + output."]},{"cell_type":"code","source":["from time import time\n","target_output = \" a lot of data.\"\n","input_len = 3\n","\n","tic = time()\n","oi = optimise_input(base_input=True, \n","                    plt_loss=False,\n","                    verbose=2, \n","                    epochs=500, \n","                    lr_decay=False,\n","                    return_early=False, \n","                    lr=0.1, \n","                    batch_size=20, \n","                    target_output=target_output, \n","                    output_len=4,\n","                    input_len=input_len, \n","                    w_freq=20, \n","                    dist_reg=1, \n","                    perp_reg=0,\n","                    loss_type='log_prob_loss',\n","                    noise_coeff = 0.75)\n","toc = time()\n","tt = toc - tic\n","print('Time Taken: ', tt)"],"metadata":{"id":"FV8hwEsWcDgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Nk_-t5j3hdGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiments = [{'base_input': True, \n","                'plt_loss': False, \n","                'verbose': 1, \n","                'epochs': 1000, \n","                'lr_decay': False, \n","                'return_early': False, \n","                'lr': 0.1, \n","                'batch_size': 50, \n","                'target_output': ' a lot of data', \n","                'output_len': 4, \n","                'input_len': 3, \n","                'w_freq': 20, \n","                'dist_reg': 1, \n","                'perp_reg': 0, \n","                'loss_type': 'log_prob_loss',\n","                'note':''}\n","                ]\n","\n","\n","experiment_log = {}"],"metadata":{"id":"y8NBHj-uOnDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for e in experiments:\n","    tick = time()\n","    results = optimise_input(**e)\n","    tock = time()\n","    rt = tock - tick\n","    results.update({'runtime':rt})\n","    \n","    with open(\"backwards_results.json\",\"a\") as f:\n","        f.write(json.dumps({tick:results}))\n"],"metadata":{"id":"81r4gT4m2NQ1","colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"status":"error","timestamp":1673450415288,"user_tz":0,"elapsed":1032,"user":{"displayName":"The Inamorata Press","userId":"11358823481707850507"}},"outputId":"3f770512-8d37-4fc6-fa83-0f6fe8195e9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimising input of length 3 to maximise output logits for \" a lot of data\"\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-1d84170c1650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimise_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtock\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-83342ba9769a>\u001b[0m in \u001b[0;36moptimise_input\u001b[0;34m(epochs, lr, rand_after, w_freq, base_input, batch_size, input_len, target_output, output_len, dist_reg, perp_reg, plt_loss, loss_type, seed, return_early, verbose, lr_decay, noise_coeff, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;31m# output_len < output_ix.shape[0] was dealt with in line 135\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mtoken_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;31m# As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# legal token embedding for each input embedding in each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-83342ba9769a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;31m# output_len < output_ix.shape[0] was dealt with in line 135\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mtoken_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;31m# As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# legal token embedding for each input embedding in each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-83342ba9769a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;31m# output_len < output_ix.shape[0] was dealt with in line 135\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mtoken_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;31m# As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# legal token embedding for each input embedding in each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-83342ba9769a>\u001b[0m in \u001b[0;36mclosest_tokens\u001b[0;34m(emb, n)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Function returns a 4-tuple (list of the n tokens, list of their indices, list of their distances from emb, and list of their embedding vectors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msorted_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# sorted_dists is a list of all embedding distances from 'emb', across entire vocab, sorted in increasing order,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 14.76 GiB total capacity; 13.74 GiB already allocated; 25.75 MiB free; 13.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["# This runs 500 batches of 50 and keeps track of the most common closest tokens to centroid embeddings, and how many appeareances they make\n","token_counts = torch.zeros(vocab_len)\n","for j in range(500):\n","    print('batch', j)\n","    centroids = kmeans(50)\n","    for i in range(50):\n","        token_counts[closest_tokens(centroids[i])[1].item()] +=1\n","\n","values, indices = torch.sort(token_counts, descending=True)\n","print(indices[:50], values[:50])"],"metadata":{"id":"442OukV-eUID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["most_common_tokens_idxs = [30212,   187,   195,   216,   182,   179,   213, 39820,   199,   124,\n","          208,   125, 23090,   554, 30208,   281,  3607,  7003,   192, 37528,\n","        15524,   217, 39752, 42089,   183,   818,   210,   201,   209,   207,\n","          211,   206,  1026,   189,   190,  1315,   219,   205,   212,   203,\n","          287,   188, 30898, 45544, 14827,   218, 30897,   202,   181, 30905]\n","most_common_tokens = [tokenizer.decode(i) for i in most_common_tokens_idxs]\n","print(most_common_tokens)\n","print(word_embeddings[30212])"],"metadata":{"id":"WI9LivBAofa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans(50)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Lx2HxfIhi4B","executionInfo":{"status":"ok","timestamp":1673452548528,"user_tz":0,"elapsed":2215,"user":{"displayName":"The Inamorata Press","userId":"11358823481707850507"}},"outputId":"811f7972-db4d-4e3e-8083-e1e3143b9d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iteration:  1 \n"," max distance between old and new centroids: 9.975008010864258\n","iteration:  2 \n"," max distance between old and new centroids: 1.3475738763809204\n","iteration:  3 \n"," max distance between old and new centroids: 1.0740748643875122\n","iteration:  4 \n"," max distance between old and new centroids: 0.8603959679603577\n","iteration:  5 \n"," max distance between old and new centroids: 0.9484898447990417\n","iteration:  6 \n"," max distance between old and new centroids: 0.6111389994621277\n","iteration:  7 \n"," max distance between old and new centroids: 0.33316004276275635\n","iteration:  8 \n"," max distance between old and new centroids: 0.25298264622688293\n","iteration:  9 \n"," max distance between old and new centroids: 0.1368548721075058\n","iteration:  10 \n"," max distance between old and new centroids: 0.1660463809967041\n","iteration:  11 \n"," max distance between old and new centroids: 0.18448098003864288\n","iteration:  12 \n"," max distance between old and new centroids: 0.3939077854156494\n","iteration:  13 \n"," max distance between old and new centroids: 0.31010088324546814\n","iteration:  14 \n"," max distance between old and new centroids: 0.2966088652610779\n","iteration:  15 \n"," max distance between old and new centroids: 0.32974058389663696\n","iteration:  16 \n"," max distance between old and new centroids: 0.28324905037879944\n","iteration:  17 \n"," max distance between old and new centroids: 0.21531817317008972\n","iteration:  18 \n"," max distance between old and new centroids: 0.1647544950246811\n","iteration:  19 \n"," max distance between old and new centroids: 0.16203825175762177\n","iteration:  20 \n"," max distance between old and new centroids: 0.18794181942939758\n","iteration:  21 \n"," max distance between old and new centroids: 0.11542848497629166\n","iteration:  22 \n"," max distance between old and new centroids: 0.1134076714515686\n","iteration:  23 \n"," max distance between old and new centroids: 0.08779381960630417\n","iteration:  24 \n"," max distance between old and new centroids: 0.06194668635725975\n","iteration:  25 \n"," max distance between old and new centroids: 0.05671731382608414\n","iteration:  26 \n"," max distance between old and new centroids: 0.06488916277885437\n","iteration:  27 \n"," max distance between old and new centroids: 0.07173579186201096\n","iteration:  28 \n"," max distance between old and new centroids: 0.087642602622509\n","iteration:  29 \n"," max distance between old and new centroids: 0.09307590872049332\n","iteration:  30 \n"," max distance between old and new centroids: 0.08441592752933502\n","iteration:  31 \n"," max distance between old and new centroids: 0.08475013077259064\n","iteration:  32 \n"," max distance between old and new centroids: 0.06981449574232101\n","iteration:  33 \n"," max distance between old and new centroids: 0.047706130892038345\n","iteration:  34 \n"," max distance between old and new centroids: 0.0323902927339077\n","iteration:  35 \n"," max distance between old and new centroids: 0.02163095213472843\n","iteration:  36 \n"," max distance between old and new centroids: 0.014823979698121548\n","iteration:  37 \n"," max distance between old and new centroids: 0.01180209405720234\n","iteration:  38 \n"," max distance between old and new centroids: 0.01028451044112444\n","iteration:  39 \n"," max distance between old and new centroids: 0.009991345927119255\n","Cluster 0  contains  1235  embeddings.\n","Cluster 1  contains  1831  embeddings.\n","Cluster 2  contains  902  embeddings.\n","Cluster 3  contains  736  embeddings.\n","Cluster 4  contains  1133  embeddings.\n","Cluster 5  contains  1502  embeddings.\n","Cluster 6  contains  2031  embeddings.\n","Cluster 7  contains  8  embeddings.\n","Cluster 8  contains  674  embeddings.\n","Cluster 9  contains  1345  embeddings.\n","Cluster 10  contains  2046  embeddings.\n","Cluster 11  contains  1250  embeddings.\n","Cluster 12  contains  5  embeddings.\n","Cluster 13  contains  730  embeddings.\n","Cluster 14  contains  2528  embeddings.\n","Cluster 15  contains  884  embeddings.\n","Cluster 16  contains  1785  embeddings.\n","Cluster 17  contains  963  embeddings.\n","Cluster 18  contains  2170  embeddings.\n","Cluster 19  contains  625  embeddings.\n","Cluster 20  contains  1270  embeddings.\n","Cluster 21  contains  1049  embeddings.\n","Cluster 22  contains  1499  embeddings.\n","Cluster 23  contains  922  embeddings.\n","Cluster 24  contains  3  embeddings.\n","Cluster 25  contains  1076  embeddings.\n","Cluster 26  contains  1628  embeddings.\n","Cluster 27  contains  691  embeddings.\n","Cluster 28  contains  1090  embeddings.\n","Cluster 29  contains  590  embeddings.\n","Cluster 30  contains  18  embeddings.\n","Cluster 31  contains  1004  embeddings.\n","Cluster 32  contains  1339  embeddings.\n","Cluster 33  contains  1110  embeddings.\n","Cluster 34  contains  655  embeddings.\n","Cluster 35  contains  1565  embeddings.\n","Cluster 36  contains  1137  embeddings.\n","Cluster 37  contains  3  embeddings.\n","Cluster 38  contains  563  embeddings.\n","Cluster 39  contains  1  embeddings.\n","Cluster 40  contains  1040  embeddings.\n","Cluster 41  contains  1379  embeddings.\n","Cluster 42  contains  568  embeddings.\n","Cluster 43  contains  1214  embeddings.\n","Cluster 44  contains  5  embeddings.\n","Cluster 45  contains  1116  embeddings.\n","Cluster 46  contains  605  embeddings.\n","Cluster 47  contains  16  embeddings.\n","Cluster 48  contains  1233  embeddings.\n","Cluster 49  contains  1485  embeddings.\n","Closest token to centroid  0 :  ['�'] 187\n","Closest token to centroid  1 :  ['\\x07'] 195\n","Closest token to centroid  2 :  [' externalToEVA'] 30212\n","Closest token to centroid  3 :  ['\\x14'] 208\n","Closest token to centroid  4 :  [' externalToEVA'] 30212\n","Closest token to centroid  5 :  ['\\x07'] 195\n","Closest token to centroid  6 :  ['\\x1c'] 216\n","Closest token to centroid  7 :  ['�'] 124\n","Closest token to centroid  8 :  [' 258'] 37528\n","Closest token to centroid  9 :  ['\\x1c'] 216\n","Closest token to centroid  10 :  [' externalToEVA'] 30212\n","Closest token to centroid  11 :  ['�'] 187\n","Closest token to centroid  12 :  ['\\x16'] 210\n","Closest token to centroid  13 :  ['\\x1c'] 216\n","Closest token to centroid  14 :  [' externalToEVA'] 30212\n","Closest token to centroid  15 :  [' externalToEVA'] 30212\n","Closest token to centroid  16 :  [' externalToEVA'] 30212\n","Closest token to centroid  17 :  ['�'] 187\n","Closest token to centroid  18 :  ['�'] 182\n","Closest token to centroid  19 :  ['�'] 179\n","Closest token to centroid  20 :  ['�'] 182\n","Closest token to centroid  21 :  [' externalToEVA'] 30212\n","Closest token to centroid  22 :  [' externalToEVA'] 30212\n","Closest token to centroid  23 :  [' externalToEVA'] 30212\n","Closest token to centroid  24 :  ['\\x13'] 207\n","Closest token to centroid  25 :  [' externalToEVA'] 30212\n","Closest token to centroid  26 :  [' externalTo'] 30208\n","Closest token to centroid  27 :  ['Although'] 7003\n","Closest token to centroid  28 :  ['龍�'] 39820\n","Closest token to centroid  29 :  ['\\x07'] 195\n","Closest token to centroid  30 :  ['\\x1c'] 216\n","Closest token to centroid  31 :  ['�'] 187\n","Closest token to centroid  32 :  [' an'] 281\n","Closest token to centroid  33 :  ['�'] 187\n","Closest token to centroid  34 :  [' externalToEVA'] 30212\n","Closest token to centroid  35 :  ['�'] 179\n","Closest token to centroid  36 :  ['\\x19'] 213\n","Closest token to centroid  37 :  ['opia'] 24464\n","Closest token to centroid  38 :  [' externalToEVA'] 30212\n","Closest token to centroid  39 :  ['SPONSORED'] 37190\n","Closest token to centroid  40 :  ['�'] 182\n","Closest token to centroid  41 :  ['�'] 187\n","Closest token to centroid  42 :  [' externalToEVA'] 30212\n","Closest token to centroid  43 :  [' externalToEVA'] 30212\n","Closest token to centroid  44 :  ['opathic'] 44650\n","Closest token to centroid  45 :  [' externalToEVA'] 30212\n","Closest token to centroid  46 :  [' externalToEVA'] 30212\n","Closest token to centroid  47 :  [' source'] 2723\n","Closest token to centroid  48 :  ['�'] 187\n","Closest token to centroid  49 :  [' externalToEVA'] 30212\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0104, -0.0040,  0.1911,  ...,  0.0722,  0.0364,  0.0326],\n","        [-0.0345, -0.0401,  0.1126,  ..., -0.0033,  0.0024,  0.0945],\n","        [ 0.0398, -0.0695,  0.1101,  ...,  0.0185,  0.0169,  0.0675],\n","        ...,\n","        [-0.0405, -0.0464,  0.1556,  ..., -0.0407, -0.0773,  0.1004],\n","        [ 0.0124, -0.0839,  0.1270,  ...,  0.0342,  0.0066,  0.0149],\n","        [ 0.0070, -0.0486,  0.0823,  ...,  0.0153, -0.0139,  0.0207]],\n","       device='cuda:0', grad_fn=<StackBackward0>)"]},"metadata":{},"execution_count":27}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/jessicamarycooper/Backwards/blob/main/Backwards.ipynb","timestamp":1673283314661}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}