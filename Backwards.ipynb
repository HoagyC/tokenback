{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/Drive')"],"metadata":{"id":"MOgggCvPZoWb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673350020519,"user_tz":0,"elapsed":19360,"user":{"displayName":"Jessica Rumbelow","userId":"14021829986331042199"}},"outputId":"8c2b3727-a564-4de2-99ed-05888d3fa381"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ar1aNn7TxI8r","colab":{"base_uri":"https://localhost:8080/","height":683,"referenced_widgets":["c3e92b903e844880817d59ab2340b294","b695dca31bbe45b0b557c37561cc444d","6e05201620ff4db092d4cfa6bc368578","721d303ee600403d84a83a17f6dbc2d0","6f6f49a7d8cc43c9a7aa23e28b17911d","4ae6416b8fa94a658e2dbd80517d0258","8345eef0cffe4bb3b91aea302713f24b","aa7f9a48e89c46a8b49d9846951886a2","6ce837a7938c4b67986efbdc21000672","a5a66bd153e140ae88f57a7ead3ea666","4798109eba3443e1b4ea1b0da463d1db","2b7911c9d8b248e692e730d93a94700c","a18f3792abf347e3954024678b3ba935","eb0aef3fcf1542d2bef52440cffd81ca","29e04cce0f08445c9ade35ad8ca002a1","133e94e183d449b48eff4ae98e70dea7","096a74f00aff4ca5ad4b2dc6e88ba679","4e72da587b664082a480d06ad8d3145f","68613c65d93e4f53944a2b06be6288bf","55e6e21de74c41698a56cd67968e4634","10d82c228c1b4ccd87a592dd2eab412d","7c21104caf8b4e609476c7fefa96fce8","683e931283cd4fd092b67208b66ad98e","4791d7f800a94bceb6a0db530c02d99f","b5df3e1a193d49df9a599e6464223d36","a0cf6acb06ba4c78816d392863941c3c","569a63f604a141d5a12d9af19e5522c9","9f6036f4f599428bbb6884771f292764","3cffaf051bef4605931d256762380346","968e85b51fb84459b689f915e0dedca8","5b2fc5a5acae4cfd8a3ad7d7f16a2d50","4301f572a2944ff39a14d75d75591c7a","f130ea575dcc438c8ed807a78518ace8","fde946f009394200aa9d813ec78d8c63","8a83491e7a454166a99c14a451c3838f","a1aa2affd0e149e58b34c3596e810e1e","e87486b98e4d4be68919448aa064aa1d","ff5eb981fffc4b7c95a1bd0ffbebc03d","69cabaa3322d49ed878907a4ca7c9dc2","7cc4ee30d5574254a13e85335f576e45","ea33f44483b14abf806dff5d810feae3","8f808a8991a14c2ba425b177668e386e","be36bdb06dc44a569504ab7afc21f3cc","f65ad7930d5c4d5385efdc0092174e1b"]},"outputId":"03b2beb8-85fd-4a98-bd37-a2d1ddbeff14","executionInfo":{"status":"ok","timestamp":1673344067787,"user_tz":0,"elapsed":45280,"user":{"displayName":"Jessica Rumbelow","userId":"14021829986331042199"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting update\n","  Downloading update-0.0.1-py2.py3-none-any.whl (2.9 kB)\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting style==1.1.0\n","  Downloading style-1.1.0-py2.py3-none-any.whl (6.4 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, style, update, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 style-1.1.0 tokenizers-0.13.2 transformers-4.25.1 update-0.0.1\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3e92b903e844880817d59ab2340b294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7911c9d8b248e692e730d93a94700c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"683e931283cd4fd092b67208b66ad98e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fde946f009394200aa9d813ec78d8c63"}},"metadata":{}}],"source":["!pip install update transformers\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, utils\n","import torch\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","from IPython import display\n","import numpy as np\n","from tqdm import tqdm\n","from time import time\n","import json\n","utils.logging.set_verbosity_error()\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","vocab_len= 50257\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left')\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id, vocab_size=vocab_len).to(device)\n","model.eval()\n","# the model will be in evaluation, not training, mode throughout\n","word_embeddings = model.transformer.wte.weight.to(device)   \n","# 'word_embeddings' tensor gives emeddings for each token in the vocab for this model,\n","# has shape (vocab_len, embedding_dimension) which in this case = (50257, 768)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCgMUuO_33Wp"},"outputs":[],"source":["(def normalise(x, min_max=[]):     \n","# normalises values of (array or tensor) x according to first (min) and second (max) values in list min_max. \n","# This effectively defaults to [0,1] if the list doesn't contain exactly two elements. \n","# The original code threw an error if min_max had length 1, so it's been changed slightly.\n","\n","# First normalise x to [0,1]\n","    rnge = x.max() - x.min()\n","    if rnge > 0:\n","        x = (x - x.min())/rnge\n","\n","# Now, if there's a min and max given in min_max list, multiply by difference and add minimum\n","    if len(min_max) > 1:\n","        rnge = min_max[1] - min_max[0]\n","        x = x * rnge + min_max[0]\n","\n","    return x\n","\n","\n","def closest_tokens(emb, n=1):      \n","# This finds the n tokens in the vocabulary that are closest in the embedding space (in terms of Euclidean distance) to a given word embedding (‘emb’).\n","# Note that here 'emb' may or may not correspond to a token (i.e., it may or may not be a 'legal' embedding).\n","# Function returns a 4-tuple (list of the n tokens, list of their indices, list of their distances from emb, and list of their embedding vectors)\n","    torch.cuda.empty_cache()\n","    dists = torch.linalg.norm(word_embeddings - emb, dim=1)\n","    sorted_dists, ix = torch.sort(dists)\t \n","    # sorted_dists is a list of all embedding distances from 'emb', across entire vocab, sorted in increasing order, \n","    # ix is a list of their corresponding 'vocab indices'\n","    tokens = [tokenizer.decode(i) for i in ix[:n]]\n","    # For each of the first n 'vocab indices' in ix, we decode it into the string version of the corresponding token. \n","    # These strings then constitute the list 'tokens'.\n","    ixs = ix[:n]\n","    dists = sorted_dists[:n]\n","    embs = word_embeddings[ixs]  # Each of these n 'embeddings' is a tensor of shape (768,)\n","    return tokens, ixs, dists, embs  \n","\n","\n","def model_emb(inputs_embeds, output_len):\n","# 'input_embeds' is a tensor of shape (batch_size, input_len, embedding_dim)\n","# 'output_len' is an integer specifying the number of output tokens to generate\n","# Note that this function doesn't involve a target output. It simply takes a tensor of input embeddings (based on input length),\n","# calculates perplexities for that batch of input sequences,\n","# and runs the batch of input sequences through GPT2, for each finding next tokens iteratively 'output_len' number of times\n","    embs = inputs_embeds   # This is going to get expanded using 'output_embs'\n","    logits = []\n","    ixs = []\n","    input_logits = None\n","    for i in range(output_len):\n","        model_out = model(inputs_embeds=embs, return_dict=True)\n","        # Does a forward pass of GPT2 (or whichever model) on a batch of inputs (given as a tensor 'embs' of embeddings).\n","        # This 'embs' will expand along its 1st dimension with each iteration.\n","        # Outputs logits and more (hidden states, attention, etc.) as a dictionary 'model_out'.\n","        # But we'll only be concerned with model_out.logits.\n","\n","        if i == 0:\n","            input_logits = model_out.logits \n","            # On first pass through loop, we simply use the logits of the model output\n","            # That's a tensor of shape (batch_size, input_len, vocab_size) giving logits for each input in each batch.\n","            # Presumably for each input, this is conditioned on the inputs that preceded it?\n","\n","        # On every pass throught the loop (including the first), we defined this tensor of shape (batch_size, 1, vocab_size):\n","        last_logits = model_out.logits[:,-1].unsqueeze(1)  \n","        # model_out.logits[:,-1] will be a 2D tensor of shape (batch_size, vocab_size), just giving logits for last input/embedding across all batches/tokens\n","        # unsqueezing, we get tensor of shape (batch_size, 1, vocab_size) also giving logits of last input/embedding, differently formatted  \n","        logits.append(last_logits)  # appends last_logits tensor to the 'logits' list \n","        ix = torch.argmax(last_logits, dim=-1)  # for each batch, finds the vocab index of the token with the largest logit in last_logits\n","        ixs.append(ix) # ...and appends this tensor of shape (batch_size,) (containing indices) it to the list 'ixs'\n","        output_embs = word_embeddings[ix]   # for each batch, finds embedding for the token with that index...\n","        embs = torch.cat([embs, output_embs], dim=1)  #...concatenates that tensor of embeddings to the 'embs' tensor in the first dimension before next iteration\n","\n","     # When the loop is completed 'embs' will be a tensor containing all of the input and output word embeddings produced by the model   \n","     # ...so presumably of shape (batch_size, input_len + output_len, embedding_dim)\n","\n","    logits = torch.cat(logits, dim=1)   # this converts logits from a list of tensors to a single tensor, by concatenating all of the tensors in the list\n","                                        # it will have shape (batch_size, output_len, vocab_size)\n","    perp = perplexity(input_logits)     # 'input_logits' was calculated on first pass through loop where only input embeddings were involved\n","    return logits, embs, perp          \n","    # logits has shape (batch_size, output_len, vocab_size),         CHECK THAT!\n","    # embs has shape (batch_size, input_len + output_len, embedding_dim)\n","    # perp has shape (batch_size,)\n","\n","\n","def perplexity(logits):\n","    # logits is of shape (batch_size, 'sequence length', vocab_size)\n","    # for all current calls, 'sequence length' is going to be input_len\n","    probs, ix = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n","    # torch.softmax(logits, dim=-1) will also be a tensor of shape (batch_size, 'sequence length', vocab_size), \n","    # but where the logits in the last dimension get converted into probabilities via softmax. torch.max() then pull out the largest of these and its index\n","    # probs is a tensor that contains the maximum probability for each token in the embedding sequence, shape (batch_size, 'sequence length')\n","    # ix is a tensor that contains the corresponding indices, also with shape (batch_size, 'sequence length')\n","    perp = 1/ (torch.prod(probs, dim=-1)**(1/probs.shape[-1])) - 1\n","    # defines a scalar that's larger with greater uncertainty (so if the probs are small, their product is small, the reciprocal of some power is large)\n","    # probs.shape[-1] is output_len; the idea of raising the probs product to power 1/output_len is to make perplexities comparable across different output lengths\n","    return perp\n","\n","\n","# Here's the key function that optimises for a sequence of input embeddings, given a target_output string:\n","def optimise_input(epochs=100, \n","                   lr=0.1, \n","                   rand_after=False,    # Do we re-initialise inputs tensor with random entries when an optimal input is found?\n","                   w_freq=10,           # logging (write) frequency\n","                   base_input=False,      # If False, start_inputs will be entirely random\n","                   batch_size=1, \n","                   input_len=1, \n","                   target_output=tokenizer.eos_token,    # Default target output is the \"end-of-string\" token; this won't generally be used\n","                   output_len=None,\n","                   dist_reg=1,       # distance regularisation coefficient\n","                   perp_reg=0,       # perplexity regularisation coefficient; setting to 0 means perplexity loss isn't a thing\n","                   plt_loss=False,   # Do we plot loss?\n","                   loss_type='log_prob_loss', \n","                   seed=0,\n","                   return_early=True,    # finishes if single optimised input is found\n","                   verbose=0,            # Controls how much info gets logged.\n","                   lr_decay=False,       # Use learning rate decay? If so, a scheduler gets invoked.\n","                   noise_coeff = 0.01, **kwargs):     # Introduced for generality in the construction of start_input[1:] below.\n","    \n","    torch.manual_seed(seed)               # sets up PyTorch random number generator\n","\n","    results = {'args': locals()}\n","\n","    if plt_loss:\n","        plt.rcParams.update({'figure.figsize': (40,6)})\n","\n","    total_losses = []\n","    losses = []\n","    dists = []\n","    perps = []\n","    optimised_inputs = dict()\n","    done = None\n","\n","    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n","    # output_ix is a 1-D tensor of shape (output_len,) that contains the indices of the tokens in the encoding of the string 'target_output'\n","    # tokenizer.encode(target_output, return_tensors='pt') is a list containing this one tensor, hence the need for the [0]\n","    # \"return_tensors='pt'\" ensures that we get a tensor in PyTorch format\n","\n","    if output_len == None or output_len < output_ix.shape[0]:                    # This won't generally be the case, but if we don't specify output_len (i.e. it's == None), then...\n","        output_len = output_ix.shape[0]       # ...it will be set to the number of tokens in the encoding of the string 'target_output'\n","    # Why not just set output_len = output_ix.shape[0] in all cases?\n","    # Will there be situations where we want output_len to be of a different size to the number of tokens in target_output?\n","\n","    print('Optimising input of length {} to maximise output logits for \"{}\"'.format(input_len, target_output))\n","    # Typically this would print something like 'Optimising input of length 6 to maximise output logits for \"KILL ALL HUMANS!\"'.\n","\n","    if base_input == False:\n","        start_input = torch.rand(batch_size, input_len, word_embeddings.shape[-1]).to(device)\n","        # If no base_input is provided, we construct start_input as a random tensor \n","        # of shape (batch_size, input_len, embedding_dim) (embedding_dim = 768 for this GPT-2 model).\n","        start_input = normalise(start_input,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n","        # We normalise this random tensor so that its minimum and maximum values correspond to those in the entire word_embeddings tensor\n","        # This dispenses with whole swathes of \"input space\" which contain no legal token embeddings \n","        # (we're limiting ourselves to a kind of \"hull\" defined by the 50527 vocab tokens in the embedding space), \n","        # which is a sensible place to look for optimised inputs.\n","    else:\n","        start_input = word_embeddings[output_ix].mean(dim=0).repeat(batch_size, input_len, 1)\n","\n","        if batch_size > 1:\n","            start_input[1:] += (torch.rand_like(start_input[1:]) + torch.full_like(start_input[1:], -0.5)) * noise_coeff\n","        #...and if we have more than one element in our batch, we \"noise\" the rest. \n","        # This was originally done using \"*=\" (multiplying entries by small random numbers)\n","        # We've changed this to \"+=\" (adding  small random numbers instead of multiplying by them).\n","        # The original code would have pushed everything in a positive direction, hence the use of a tensor full of -0.5's.       \n","\n","    \n","    input = torch.nn.Parameter(start_input, requires_grad=True)\n","    # input is not a tensor, it's a Parameter object that wraps a tensor and adds additional functionality. \n","    # 'input.data' is used below\n","    \n","    optimiser = torch.optim.Adam([input], lr=lr)\n","    # standard optimiser; note that it generally operates on a list of tensors, so we're giving it a list of one tensor; standard learning rate\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5)\n","    # this is used when loss hasn't improved for 20 timesteps; this scheduler will reduce the lr by a 'factor' of 0.5 when the \n","    # validation loss stops improving for 'patience' (here 20) epochs, and will wait 'cooldown' (here 20) epochs before resuming normal operation.\n","\n","    # now we loop across training epochs\n","    for e in range(epochs):\n","\n","        logits, emb, perp = model_emb(torch.clamp(input, word_embeddings.min(), word_embeddings.max()), output_len)\n","        # Does forward pass on a 'clamped' version of the 'input' tensor (done to contain it within the 'hull' of the vocabulary within 'input space').\n","        # Iterates to produce an output of output_len tokens, \n","        # returns: 'logits' = tensor of logits for output, of shape (batch_size, output_len, vocab_size)\n","        # 'emb': tensor of embeddings for input+output of shape (batch_size, input_len + output_len, embedding_dim); \n","        # 'perp': the input sequence perplexities tensor, of shape (batch_size,)\n","        probs = torch.softmax(logits, dim=-1)\n","        # For each batch, output, converts the sequence of logits (of length 'vocab_size') in the 'logits' tensor to probabilities, using softmax\n","\n","        logits = (logits - logits.min(dim=-1)[0].unsqueeze(-1)) / (logits.max(dim=-1)[0].unsqueeze(-1) - logits.min(dim=-1)[0].unsqueeze(-1))\n","        # This appears to be normalising the logits for each batch/output embedding so they're all between 0 and 1... \n","        # This is for ease of visualisation.\n","\n","        perp_loss = perp.mean() * perp_reg\n","        # That's taking the mean perp value across all batches, then regularising it. Currently perp_reg is set to 0, so perp_loss = 0.\n","\n","        if output_len > output_ix.shape[0]:\n","            target_logits = torch.stack([logits[:, :, ix] for ix in output_ix], dim=-1)\n","            target_logits = torch.max(target_logits, dim=-1)[0]\n","            # logits is shape (batch_size, output_len, vocab_size) \n","            # We throw out everything in the final dimension except those logits corresponding to indices of tokens in the target_ouput\n","            # This gives tensor with shape (batch_size, output_len, output_ix.shape[0])\n","            # We then take the maximum of those for each batch, output; this gives shape (batch_size, output_len)\n","            # The [0] returns just the max (torch.max returns (max, indices) tuple)\n","            target_probs = torch.stack([probs[:, :, ix] for ix in output_ix], dim=-1)\n","            target_probs = torch.max(target_probs, dim=-1)[0]\n","            # This does the analogous thing for probs.\n","\n","        else:\n","            target_logits = torch.stack([logits[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n","            target_probs = torch.stack([probs[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n","            # This handles case where output_len == output_ix.shape[0]\n","            # target_logits now of shape (batch_size, output_len)?\n","            # output_len < output_ix.shape[0] was dealt with in line 135\n","            \n","        token_dist = torch.stack([torch.stack([closest_tokens(e)[2].squeeze(-1) for e in input[b]]) for b in range(batch_size)])\n","        # As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\n","        # legal token embedding for each input embedding in each batch\n","        mean_token_dist = token_dist.mean() * dist_reg\n","        # A single scalar value, taking mean across the batch and input embeddings? \n","\n","\n","        # There are currently four loss types, many more could be introduced.\n","        # log_prob_loss is the current default.\n","        if loss_type == 'logit_loss':\n","            loss = 1-target_logits\n","        elif loss_type == 'log_prob_loss':\n","            loss = -torch.log(target_probs)\n","        elif loss_type == 'prob_loss':\n","            loss = 1-target_probs\n","        elif loss_type == 'CE':\n","            loss = torch.nn.functional.cross_entropy(logits.swapaxes(-1,-2), output_ix.repeat(batch_size, 1), reduction=None)\n","        else:\n","            print(loss_type + 'is not implemented.')\n","            return \n","\n","        batch_loss = loss.mean()\n","\n","        total_loss = torch.stack([mean_token_dist, batch_loss, perp_loss]).mean()\n","        # This is this just (mean_token_dist + loss + perp_loss)/3 tensorised across batches, yes?\n","\n","        total_losses.append(total_loss.detach().cpu().data)\n","        losses.append(batch_loss.detach().cpu().data)\n","        dists.append(mean_token_dist.detach().cpu().data)\n","        perps.append(perp_loss.detach().cpu().data)\n","        # these four lists were intialised above. We're appeneding to the list each epoch. All are scalars.\n","\n","        closest_ix = torch.stack([torch.stack([closest_tokens(e)[1] for e in b]) for b in input]).squeeze(-1)\n","        # This is similar to above, but building a tensor of indices of nearest embeddings, rather than distances.\n","        # Iterates over batches, and for each batch iterates over embeddings, giving tensor of shape (batch_size, input_len).\n","\n","        model_outs = model.generate(closest_ix, max_length = output_len+input_len)\n","        # The 'closest_ix' tensor is passed as the initial input sequence to the model, \n","        # and the max_length parameter specifies the maximum length of the total sequence to generate.\n","        # The output sequence will be terminated either when the end-of-sequence token is generated \n","        # or when the maximum length is reached, whichever occurs first.\n","        # \n","        # The output of the model.generate method will be a tuple containing the generated sequences and the model's internal states. \n","        # The generated sequences will be stored in a tensor of shape (batch_size, output_len+input_len). \n","        # Each element of the tensor will be a sequence of tokens with a length of at most output_len+input_len.\n","        \n","        for b in range(batch_size):\n","\n","            if output_len > output_ix.shape[0]:\n","                if target_output in tokenizer.decode(model_outs[b][input_len:]):\n","                    done = tokenizer.decode(model_outs[b][:input_len])\n","                    optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n","                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n","                # we decode these as tokens... is the target_output a substring?\n","                # if so, we print the target_output and the decoded string that contains it\n","                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n","\n","\n","            if tokenizer.decode(model_outs[b][input_len:]) == target_output:\n","                done = tokenizer.decode(model_outs[b][:input_len])\n","                optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n","                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n","                # we decode these as tokens... is the target_output equal to output string?\n","                # Nothing printed in this case.\n","                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n","            \n","            if done is not None and rand_after:\n","                input.data[b] = torch.rand_like(input[b])\n","                # Random re-initialisation (if 'rand_after' set to True)\n","\n","  \n","        if ((e+1) % w_freq == 0) or done and return_early:\n","            display.clear_output(wait=True) \n","            print('\\033[H\\033[J')  \n","        # Every w epochs we write to log, unless we have found an optimised input before that and 'return_early' == True. \n","        # I'm still not entirely sure about the idea of 'return_early'.\n","\n","            if plt_loss:\n","                plt.plot(range(len(total_losses)), total_losses, label='Total Loss', color='black')\n","                plt.plot(range(len(losses)), losses, label='Output Loss')\n","                plt.plot(range(len(dists)), dists, label='Emb Dist Loss')\n","                plt.plot(range(len(perps)), perps, label='Perp Loss')\n","                plt.yscale('log')\n","                plt.legend()\n","\n","                plt.show()\n","\n","            print('Inputs found: ', optimised_inputs)\n","            print('{}/{} Output Loss: {} Emb Dist Loss: {} Perp Loss: {} LR: {}'.format(e+1, epochs, batch_loss, mean_token_dist, perp_loss, optimiser.param_groups[0]['lr']))\n","            if verbose == 3:\n","                print('Target Probs: {}\\nTarget Logits: {}\\nInput Dists: {}\\nInput Perplexity: {}\\n'.format(target_probs.detach().cpu().numpy(), target_logits.detach().cpu().numpy(), token_dist.detach().cpu().numpy(), perp.detach().reshape(-1).cpu().numpy()))\n","            # Optimised inputs and additional information are printed as part of log\n","\n","            for b in range(batch_size):\n","                if verbose > 0:\n","                    if verbose == 2:\n","                        print(b, repr(' Raw embeddings: {}'.format(''.join([closest_tokens(e)[0][0] for e in emb[b]]))))\n","                        # Change name to clarify (output of model if we just put in raw embeddings)\n","                        # prints batch number; closest_tokens(e)[0] is a list of tokens, closest_tokens(e)[0] is the first (closest) of these\n","                        # these get joined with separator '' (SHOULDN'T THAT BE ' '?)  \n","                    print(b, repr(' Closest embeddings: {}'.format(tokenizer.decode(model_outs[b]), '\\n')))\n","                        # WON'T THIS give string decodings of the embeddings, rather than the embeddings themselves?\n","                else:\n","                    print(repr(tokenizer.decode(model_outs[b])), end=' ')\n","                    # The least verbose printed output. The 'end' parameter is used to specify the end-of-line string that is appended to the output. \n","                    # By default, this is a newline character, but in this case it has been set to a single space character, \n","                    # so the output will be separated by spaces rather than newlines.\n","\n","            if done and return_early:\n","                print('\\nOptimised Input: \"{}\"'.format(done))\n","                results['optimised_inputs'] = optimised_inputs\n","                return results\n","                # we know optimised_inputs set contains a single element in this case\n","            \n","        optimiser.zero_grad()\n","        total_loss.backward()\n","        optimiser.step()\n","        # I assume these three lines are standard NN optimisation stuff?\n","\n","        if lr_decay:\n","            scheduler.step(total_loss)\n","         # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5) gets used if lr_decay == True\n","        done = None\n","\n","    results['optimised_inputs'] = optimised_inputs\n","    return results\n","    # that's a set of strings\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tev0cDbkdbxO","outputId":"d0d86b3c-4671-4ead-8793-0eab7246b14c","executionInfo":{"status":"ok","timestamp":1673347282387,"user_tz":0,"elapsed":718,"user":{"displayName":"Jessica Rumbelow","userId":"14021829986331042199"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[9246, 9529, 22009, 351]\n","['cat', ' rh', 'ymes', ' with']\n","4\n","cat rhymes with \"I\n"]}],"source":["ix = tokenizer.encode(\"\")\n","# list of 'vocab indices'\n","print(ix)\n","print([tokenizer.decode(i) for i in ix])\n","# prints reconstruction of input string\n","print(len(ix))\n","# prints number of tokens\n","output_len=2\n","model_out = model.generate(torch.tensor(ix).unsqueeze(0).to(device), max_length = output_len + len(ix))\n","print(tokenizer.decode(model_out[0]))\n","# pushes input string throught GPT2 (or whichever model) iteratively producing output_len number of tokens, then prints input + output."]},{"cell_type":"code","source":["experiments = [{'base_input': False, \n","                'plt_loss': False, \n","                'verbose': 1, \n","                'epochs': 1000, \n","                'lr_decay': False, \n","                'return_early': False, \n","                'lr': 0.1, \n","                'batch_size': 20, \n","                'target_output': ' a lot of data', \n","                'output_len': 4, \n","                'input_len': 3, \n","                'w_freq': 20, \n","                'dist_reg': 1, \n","                'perp_reg': 0, \n","                'loss_type': 'log_prob_loss',\n","                'note':''}\n","                ]\n","\n","\n","experiment_log = {}"],"metadata":{"id":"y8NBHj-uOnDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for e in experiments:\n","    tick = time()\n","    results = optimise_input(**e)\n","    tock = time()\n","    rt = tock - tick\n","    results.update({'runtime':rt})\n","    \n","    with open(\"backwards_results.json\",\"a\") as f:\n","        f.write(json.dumps({tick:results}))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"81r4gT4m2NQ1","outputId":"f858cf98-9501-4fbe-885e-a43e6fc03e5c","executionInfo":{"status":"error","timestamp":1673355328822,"user_tz":0,"elapsed":198,"user":{"displayName":"Jessica Rumbelow","userId":"14021829986331042199"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1d84170c1650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimise_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtock\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'experiments' is not defined"]}]},{"cell_type":"code","source":["    "],"metadata":{"id":"2jZJMakszJ_U"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/jessicamarycooper/Backwards/blob/main/Backwards.ipynb","timestamp":1673283314661}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c3e92b903e844880817d59ab2340b294":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b695dca31bbe45b0b557c37561cc444d","IPY_MODEL_6e05201620ff4db092d4cfa6bc368578","IPY_MODEL_721d303ee600403d84a83a17f6dbc2d0"],"layout":"IPY_MODEL_6f6f49a7d8cc43c9a7aa23e28b17911d"}},"b695dca31bbe45b0b557c37561cc444d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ae6416b8fa94a658e2dbd80517d0258","placeholder":"​","style":"IPY_MODEL_8345eef0cffe4bb3b91aea302713f24b","value":"Downloading: 100%"}},"6e05201620ff4db092d4cfa6bc368578":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa7f9a48e89c46a8b49d9846951886a2","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ce837a7938c4b67986efbdc21000672","value":1042301}},"721d303ee600403d84a83a17f6dbc2d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5a66bd153e140ae88f57a7ead3ea666","placeholder":"​","style":"IPY_MODEL_4798109eba3443e1b4ea1b0da463d1db","value":" 1.04M/1.04M [00:01&lt;00:00, 1.07MB/s]"}},"6f6f49a7d8cc43c9a7aa23e28b17911d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ae6416b8fa94a658e2dbd80517d0258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8345eef0cffe4bb3b91aea302713f24b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa7f9a48e89c46a8b49d9846951886a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ce837a7938c4b67986efbdc21000672":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a5a66bd153e140ae88f57a7ead3ea666":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4798109eba3443e1b4ea1b0da463d1db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b7911c9d8b248e692e730d93a94700c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a18f3792abf347e3954024678b3ba935","IPY_MODEL_eb0aef3fcf1542d2bef52440cffd81ca","IPY_MODEL_29e04cce0f08445c9ade35ad8ca002a1"],"layout":"IPY_MODEL_133e94e183d449b48eff4ae98e70dea7"}},"a18f3792abf347e3954024678b3ba935":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_096a74f00aff4ca5ad4b2dc6e88ba679","placeholder":"​","style":"IPY_MODEL_4e72da587b664082a480d06ad8d3145f","value":"Downloading: 100%"}},"eb0aef3fcf1542d2bef52440cffd81ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68613c65d93e4f53944a2b06be6288bf","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55e6e21de74c41698a56cd67968e4634","value":456318}},"29e04cce0f08445c9ade35ad8ca002a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10d82c228c1b4ccd87a592dd2eab412d","placeholder":"​","style":"IPY_MODEL_7c21104caf8b4e609476c7fefa96fce8","value":" 456k/456k [00:01&lt;00:00, 577kB/s]"}},"133e94e183d449b48eff4ae98e70dea7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"096a74f00aff4ca5ad4b2dc6e88ba679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e72da587b664082a480d06ad8d3145f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68613c65d93e4f53944a2b06be6288bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55e6e21de74c41698a56cd67968e4634":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10d82c228c1b4ccd87a592dd2eab412d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c21104caf8b4e609476c7fefa96fce8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"683e931283cd4fd092b67208b66ad98e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4791d7f800a94bceb6a0db530c02d99f","IPY_MODEL_b5df3e1a193d49df9a599e6464223d36","IPY_MODEL_a0cf6acb06ba4c78816d392863941c3c"],"layout":"IPY_MODEL_569a63f604a141d5a12d9af19e5522c9"}},"4791d7f800a94bceb6a0db530c02d99f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f6036f4f599428bbb6884771f292764","placeholder":"​","style":"IPY_MODEL_3cffaf051bef4605931d256762380346","value":"Downloading: 100%"}},"b5df3e1a193d49df9a599e6464223d36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_968e85b51fb84459b689f915e0dedca8","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b2fc5a5acae4cfd8a3ad7d7f16a2d50","value":665}},"a0cf6acb06ba4c78816d392863941c3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4301f572a2944ff39a14d75d75591c7a","placeholder":"​","style":"IPY_MODEL_f130ea575dcc438c8ed807a78518ace8","value":" 665/665 [00:00&lt;00:00, 53.0kB/s]"}},"569a63f604a141d5a12d9af19e5522c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f6036f4f599428bbb6884771f292764":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cffaf051bef4605931d256762380346":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"968e85b51fb84459b689f915e0dedca8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b2fc5a5acae4cfd8a3ad7d7f16a2d50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4301f572a2944ff39a14d75d75591c7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f130ea575dcc438c8ed807a78518ace8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fde946f009394200aa9d813ec78d8c63":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a83491e7a454166a99c14a451c3838f","IPY_MODEL_a1aa2affd0e149e58b34c3596e810e1e","IPY_MODEL_e87486b98e4d4be68919448aa064aa1d"],"layout":"IPY_MODEL_ff5eb981fffc4b7c95a1bd0ffbebc03d"}},"8a83491e7a454166a99c14a451c3838f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69cabaa3322d49ed878907a4ca7c9dc2","placeholder":"​","style":"IPY_MODEL_7cc4ee30d5574254a13e85335f576e45","value":"Downloading: 100%"}},"a1aa2affd0e149e58b34c3596e810e1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea33f44483b14abf806dff5d810feae3","max":548118077,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f808a8991a14c2ba425b177668e386e","value":548118077}},"e87486b98e4d4be68919448aa064aa1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be36bdb06dc44a569504ab7afc21f3cc","placeholder":"​","style":"IPY_MODEL_f65ad7930d5c4d5385efdc0092174e1b","value":" 548M/548M [00:06&lt;00:00, 83.5MB/s]"}},"ff5eb981fffc4b7c95a1bd0ffbebc03d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69cabaa3322d49ed878907a4ca7c9dc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cc4ee30d5574254a13e85335f576e45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea33f44483b14abf806dff5d810feae3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f808a8991a14c2ba425b177668e386e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be36bdb06dc44a569504ab7afc21f3cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f65ad7930d5c4d5385efdc0092174e1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}