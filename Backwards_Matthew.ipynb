{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessicamarycooper/Backwards/blob/main/Backwards_Matthew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ],
      "metadata": {
        "id": "MOgggCvPZoWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "dfbcd385-2e71-4072-b309-159866574fcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c08de23972c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar1aNn7TxI8r"
      },
      "outputs": [],
      "source": [
        "!pip install update transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, utils\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import json\n",
        "utils.logging.set_verbosity_error()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vocab_len= 50257\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left')\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id, vocab_size=vocab_len).to(device)\n",
        "model.eval()\n",
        "# the model will be in evaluation, not training, mode throughout\n",
        "word_embeddings = model.transformer.wte.weight.to(device)   \n",
        "# 'word_embeddings' tensor gives emeddings for each token in the vocab for this model,\n",
        "# has shape (vocab_len, embedding_dimension) which in this case = (50257, 768)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qCgMUuO_33Wp"
      },
      "outputs": [],
      "source": [
        "def normalise(x, min_max=[]):     \n",
        "# normalises values of (array or tensor) x according to first (min) and second (max) values in list min_max. \n",
        "# This effectively defaults to [0,1] if the list doesn't contain exactly two elements. \n",
        "# The original code threw an error if min_max had length 1, so it's been changed slightly.\n",
        "\n",
        "# First normalise x to [0,1]\n",
        "    rnge = x.max() - x.min()\n",
        "    if rnge > 0:\n",
        "        x = (x - x.min())/rnge\n",
        "\n",
        "# Now, if there's a min and max given in min_max list, multiply by difference and add minimum\n",
        "    if len(min_max) > 1:\n",
        "        rnge = min_max[1] - min_max[0]\n",
        "        x = x * rnge + min_max[0]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def closest_tokens(emb, n=1):      \n",
        "# This finds the n tokens in the vocabulary that are closest in the embedding space (in terms of Euclidean distance) to a given word embedding (‘emb’).\n",
        "# Note that here 'emb' may or may not correspond to a token (i.e., it may or may not be a 'legal' embedding).\n",
        "# Function returns a 4-tuple (list of the n tokens, list of their indices, list of their distances from emb, and list of their embedding vectors)\n",
        "    torch.cuda.empty_cache()\n",
        "    dists = torch.linalg.norm(word_embeddings - emb, dim=1)\n",
        "    sorted_dists, ix = torch.sort(dists)\t \n",
        "    # sorted_dists is a list of all embedding distances from 'emb', across entire vocab, sorted in increasing order, \n",
        "    # ix is a list of their corresponding 'vocab indices'\n",
        "    tokens = [tokenizer.decode(i) for i in ix[:n]]\n",
        "    # For each of the first n 'vocab indices' in ix, we decode it into the string version of the corresponding token. \n",
        "    # These strings then constitute the list 'tokens'.\n",
        "    ixs = ix[:n]\n",
        "    dists = sorted_dists[:n]\n",
        "    embs = word_embeddings[ixs]  # Each of these n 'embeddings' is a tensor of shape (768,)\n",
        "    return tokens, ixs, dists, embs  \n",
        "\n",
        "\n",
        "def model_emb(inputs_embeds, output_len):\n",
        "# 'input_embeds' is a tensor of shape (batch_size, input_len, embedding_dim)\n",
        "# 'output_len' is an integer specifying the number of output tokens to generate\n",
        "# Note that this function doesn't involve a target output. It simply takes a tensor of input embeddings (based on input length),\n",
        "# calculates perplexities for that batch of input sequences,\n",
        "# and runs the batch of input sequences through GPT2, for each finding next tokens iteratively 'output_len' number of times\n",
        "    embs = inputs_embeds   # This is going to get expanded using 'output_embs'\n",
        "    logits = []\n",
        "    ixs = []\n",
        "    input_logits = None\n",
        "    for i in range(output_len):\n",
        "        model_out = model(inputs_embeds=embs, return_dict=True)\n",
        "        # Does a forward pass of GPT2 (or whichever model) on a batch of inputs (given as a tensor 'embs' of embeddings).\n",
        "        # This 'embs' will expand along its 1st dimension with each iteration.\n",
        "        # Outputs logits and more (hidden states, attention, etc.) as a dictionary 'model_out'.\n",
        "        # But we'll only be concerned with model_out.logits.\n",
        "\n",
        "        if i == 0:\n",
        "            input_logits = model_out.logits \n",
        "            # On first pass through loop, we simply use the logits of the model output\n",
        "            # That's a tensor of shape (batch_size, input_len, vocab_size) giving logits for each input in each batch.\n",
        "            # Presumably for each input, this is conditioned on the inputs that preceded it?\n",
        "\n",
        "        # On every pass throught the loop (including the first), we defined this tensor of shape (batch_size, 1, vocab_size):\n",
        "        last_logits = model_out.logits[:,-1].unsqueeze(1)  \n",
        "        # model_out.logits[:,-1] will be a 2D tensor of shape (batch_size, vocab_size), just giving logits for last input/embedding across all batches/tokens\n",
        "        # unsqueezing, we get tensor of shape (batch_size, 1, vocab_size) also giving logits of last input/embedding, differently formatted  \n",
        "        logits.append(last_logits)  # appends last_logits tensor to the 'logits' list \n",
        "        ix = torch.argmax(last_logits, dim=-1)  # for each batch, finds the vocab index of the token with the largest logit in last_logits\n",
        "        ixs.append(ix) # ...and appends this tensor of shape (batch_size,) (containing indices) it to the list 'ixs'\n",
        "        output_embs = word_embeddings[ix]   # for each batch, finds embedding for the token with that index...\n",
        "        embs = torch.cat([embs, output_embs], dim=1)  #...concatenates that tensor of embeddings to the 'embs' tensor in the first dimension before next iteration\n",
        "\n",
        "     # When the loop is completed 'embs' will be a tensor containing all of the input and output word embeddings produced by the model   \n",
        "     # ...so presumably of shape (batch_size, input_len + output_len, embedding_dim)\n",
        "\n",
        "    logits = torch.cat(logits, dim=1)   # this converts logits from a list of tensors to a single tensor, by concatenating all of the tensors in the list\n",
        "                                        # it will have shape (batch_size, output_len, vocab_size)\n",
        "    perp = perplexity(input_logits)     # 'input_logits' was calculated on first pass through loop where only input embeddings were involved\n",
        "    return logits, embs, perp          \n",
        "    # logits has shape (batch_size, output_len, vocab_size),         CHECK THAT!\n",
        "    # embs has shape (batch_size, input_len + output_len, embedding_dim)\n",
        "    # perp has shape (batch_size,)\n",
        "\n",
        "\n",
        "def perplexity(logits):\n",
        "    # logits is of shape (batch_size, 'sequence length', vocab_size)\n",
        "    # for all current calls, 'sequence length' is going to be input_len\n",
        "    probs, ix = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n",
        "    # torch.softmax(logits, dim=-1) will also be a tensor of shape (batch_size, 'sequence length', vocab_size), \n",
        "    # but where the logits in the last dimension get converted into probabilities via softmax. torch.max() then pull out the largest of these and its index\n",
        "    # probs is a tensor that contains the maximum probability for each token in the embedding sequence, shape (batch_size, 'sequence length')\n",
        "    # ix is a tensor that contains the corresponding indices, also with shape (batch_size, 'sequence length')\n",
        "    perp = 1/ (torch.prod(probs, dim=-1)**(1/probs.shape[-1])) - 1\n",
        "    # defines a scalar that's larger with greater uncertainty (so if the probs are small, their product is small, the reciprocal of some power is large)\n",
        "    # probs.shape[-1] is output_len; the idea of raising the probs product to power 1/output_len is to make perplexities comparable across different output lengths\n",
        "    return perp\n",
        "\n",
        "\n",
        "# Here's the key function that optimises for a sequence of input embeddings, given a target_output string:\n",
        "def optimise_input(epochs=100, \n",
        "                   lr=0.1, \n",
        "                   rand_after=False,    # Do we re-initialise inputs tensor with random entries when an optimal input is found?\n",
        "                   w_freq=10,           # logging (write) frequency\n",
        "                   base_input=False,      # If False, start_inputs will be entirely random\n",
        "                   batch_size=1, \n",
        "                   input_len=1, \n",
        "                   target_output=tokenizer.eos_token,    # Default target output is the \"end-of-string\" token; this won't generally be used\n",
        "                   output_len=None,\n",
        "                   dist_reg=1,       # distance regularisation coefficient\n",
        "                   perp_reg=0,       # perplexity regularisation coefficient; setting to 0 means perplexity loss isn't a thing\n",
        "                   plt_loss=False,   # Do we plot loss?\n",
        "                   loss_type='log_prob_loss', \n",
        "                   seed=0,\n",
        "                   return_early=True,    # finishes if single optimised input is found\n",
        "                   verbose=0,            # Controls how much info gets logged.\n",
        "                   lr_decay=False,       # Use learning rate decay? If so, a scheduler gets invoked.\n",
        "                   noise_coeff = 0.01, **kwargs):     # Introduced for generality in the construction of start_input[1:] below.\n",
        "    \n",
        "    torch.manual_seed(seed)               # sets up PyTorch random number generator\n",
        "\n",
        "    results = {'args': locals()}\n",
        "\n",
        "    if plt_loss:\n",
        "        plt.rcParams.update({'figure.figsize': (40,6)})\n",
        "\n",
        "    total_losses = []\n",
        "    losses = []\n",
        "    dists = []\n",
        "    perps = []\n",
        "    optimised_inputs = dict()\n",
        "    done = None\n",
        "\n",
        "    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n",
        "    # output_ix is a 1-D tensor of shape (output_len,) that contains the indices of the tokens in the encoding of the string 'target_output'\n",
        "    # tokenizer.encode(target_output, return_tensors='pt') is a list containing this one tensor, hence the need for the [0]\n",
        "    # \"return_tensors='pt'\" ensures that we get a tensor in PyTorch format\n",
        "\n",
        "    if output_len == None or output_len < output_ix.shape[0]:                    # This won't generally be the case, but if we don't specify output_len (i.e. it's == None), then...\n",
        "        output_len = output_ix.shape[0]       # ...it will be set to the number of tokens in the encoding of the string 'target_output'\n",
        "    # Why not just set output_len = output_ix.shape[0] in all cases?\n",
        "    # Will there be situations where we want output_len to be of a different size to the number of tokens in target_output?\n",
        "\n",
        "    print('Optimising input of length {} to maximise output logits for \"{}\"'.format(input_len, target_output))\n",
        "    # Typically this would print something like 'Optimising input of length 6 to maximise output logits for \"KILL ALL HUMANS!\"'.\n",
        "\n",
        "    if base_input == False:\n",
        "        start_input = torch.rand(batch_size, input_len, word_embeddings.shape[-1]).to(device)\n",
        "        # If no base_input is provided, we construct start_input as a random tensor \n",
        "        # of shape (batch_size, input_len, embedding_dim) (embedding_dim = 768 for this GPT-2 model).\n",
        "        start_input = normalise(start_input,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n",
        "        # We normalise this random tensor so that its minimum and maximum values correspond to those in the entire word_embeddings tensor\n",
        "        # This dispenses with whole swathes of \"input space\" which contain no legal token embeddings \n",
        "        # (we're limiting ourselves to a kind of \"hull\" defined by the 50527 vocab tokens in the embedding space), \n",
        "        # which is a sensible place to look for optimised inputs.\n",
        "    else:\n",
        "        start_input = word_embeddings[output_ix].mean(dim=0).repeat(batch_size, input_len, 1)\n",
        "\n",
        "        if batch_size > 1:\n",
        "            start_input[1:] += (torch.rand_like(start_input[1:]) + torch.full_like(start_input[1:], -0.5)) * noise_coeff\n",
        "        #...and if we have more than one element in our batch, we \"noise\" the rest. \n",
        "        # This was originally done using \"*=\" (multiplying entries by small random numbers)\n",
        "        # We've changed this to \"+=\" (adding  small random numbers instead of multiplying by them).\n",
        "        # The original code would have pushed everything in a positive direction, hence the use of a tensor full of -0.5's.       \n",
        "\n",
        "    \n",
        "    input = torch.nn.Parameter(start_input, requires_grad=True)\n",
        "    # input is not a tensor, it's a Parameter object that wraps a tensor and adds additional functionality. \n",
        "    # 'input.data' is used below\n",
        "    \n",
        "    optimiser = torch.optim.Adam([input], lr=lr)\n",
        "    # standard optimiser; note that it generally operates on a list of tensors, so we're giving it a list of one tensor; standard learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5)\n",
        "    # this is used when loss hasn't improved for 20 timesteps; this scheduler will reduce the lr by a 'factor' of 0.5 when the \n",
        "    # validation loss stops improving for 'patience' (here 20) epochs, and will wait 'cooldown' (here 20) epochs before resuming normal operation.\n",
        "\n",
        "    # now we loop across training epochs\n",
        "    for e in range(epochs):\n",
        "\n",
        "        logits, emb, perp = model_emb(torch.clamp(input, word_embeddings.min(), word_embeddings.max()), output_len)\n",
        "        # Does forward pass on a 'clamped' version of the 'input' tensor (done to contain it within the 'hull' of the vocabulary within 'input space').\n",
        "        # Iterates to produce an output of output_len tokens, \n",
        "        # returns: 'logits' = tensor of logits for output, of shape (batch_size, output_len, vocab_size)\n",
        "        # 'emb': tensor of embeddings for input+output of shape (batch_size, input_len + output_len, embedding_dim); \n",
        "        # 'perp': the input sequence perplexities tensor, of shape (batch_size,)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        # For each batch, output, converts the sequence of logits (of length 'vocab_size') in the 'logits' tensor to probabilities, using softmax\n",
        "\n",
        "        logits = (logits - logits.min(dim=-1)[0].unsqueeze(-1)) / (logits.max(dim=-1)[0].unsqueeze(-1) - logits.min(dim=-1)[0].unsqueeze(-1))\n",
        "        # This appears to be normalising the logits for each batch/output embedding so they're all between 0 and 1... \n",
        "        # This is for ease of visualisation.\n",
        "\n",
        "        perp_loss = perp.mean() * perp_reg\n",
        "        # That's taking the mean perp value across all batches, then regularising it. Currently perp_reg is set to 0, so perp_loss = 0.\n",
        "\n",
        "        if output_len > output_ix.shape[0]:\n",
        "            target_logits = torch.stack([logits[:, :, ix] for ix in output_ix], dim=-1)\n",
        "            target_logits = torch.max(target_logits, dim=-1)[0]\n",
        "            # logits is shape (batch_size, output_len, vocab_size) \n",
        "            # We throw out everything in the final dimension except those logits corresponding to indices of tokens in the target_ouput\n",
        "            # This gives tensor with shape (batch_size, output_len, output_ix.shape[0])\n",
        "            # We then take the maximum of those for each batch, output; this gives shape (batch_size, output_len)\n",
        "            # The [0] returns just the max (torch.max returns (max, indices) tuple)\n",
        "            target_probs = torch.stack([probs[:, :, ix] for ix in output_ix], dim=-1)\n",
        "            target_probs = torch.max(target_probs, dim=-1)[0]\n",
        "            # This does the analogous thing for probs.\n",
        "\n",
        "        else:\n",
        "            target_logits = torch.stack([logits[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n",
        "            target_probs = torch.stack([probs[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n",
        "            # This handles case where output_len == output_ix.shape[0]\n",
        "            # target_logits now of shape (batch_size, output_len)?\n",
        "            # output_len < output_ix.shape[0] was dealt with in line 135\n",
        "            \n",
        "        token_dist = torch.stack([torch.stack([closest_tokens(e)[2].squeeze(-1) for e in input[b]]) for b in range(batch_size)])\n",
        "        # As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\n",
        "        # legal token embedding for each input embedding in each batch\n",
        "        mean_token_dist = token_dist.mean() * dist_reg\n",
        "        # A single scalar value, taking mean across the batch and input embeddings? \n",
        "\n",
        "\n",
        "        # There are currently four loss types, many more could be introduced.\n",
        "        # log_prob_loss is the current default.\n",
        "        if loss_type == 'logit_loss':\n",
        "            loss = 1-target_logits\n",
        "        elif loss_type == 'log_prob_loss':\n",
        "            loss = -torch.log(target_probs)\n",
        "        elif loss_type == 'prob_loss':\n",
        "            loss = 1-target_probs\n",
        "        elif loss_type == 'CE':\n",
        "            loss = torch.nn.functional.cross_entropy(logits.swapaxes(-1,-2), output_ix.repeat(batch_size, 1), reduction=None)\n",
        "        else:\n",
        "            print(loss_type + 'is not implemented.')\n",
        "            return \n",
        "\n",
        "        batch_loss = loss.mean()\n",
        "\n",
        "        total_loss = torch.stack([mean_token_dist, batch_loss, perp_loss]).mean()\n",
        "        # This is this just (mean_token_dist + loss + perp_loss)/3 tensorised across batches, yes?\n",
        "\n",
        "        total_losses.append(total_loss.detach().cpu().data)\n",
        "        losses.append(batch_loss.detach().cpu().data)\n",
        "        dists.append(mean_token_dist.detach().cpu().data)\n",
        "        perps.append(perp_loss.detach().cpu().data)\n",
        "        # these four lists were intialised above. We're appeneding to the list each epoch. All are scalars.\n",
        "\n",
        "        closest_ix = torch.stack([torch.stack([closest_tokens(e)[1] for e in b]) for b in input]).squeeze(-1)\n",
        "        # This is similar to above, but building a tensor of indices of nearest embeddings, rather than distances.\n",
        "        # Iterates over batches, and for each batch iterates over embeddings, giving tensor of shape (batch_size, input_len).\n",
        "\n",
        "        model_outs = model.generate(closest_ix, max_length = output_len+input_len)\n",
        "        # The 'closest_ix' tensor is passed as the initial input sequence to the model, \n",
        "        # and the max_length parameter specifies the maximum length of the total sequence to generate.\n",
        "        # The output sequence will be terminated either when the end-of-sequence token is generated \n",
        "        # or when the maximum length is reached, whichever occurs first.\n",
        "        # \n",
        "        # The output of the model.generate method will be a tuple containing the generated sequences and the model's internal states. \n",
        "        # The generated sequences will be stored in a tensor of shape (batch_size, output_len+input_len). \n",
        "        # Each element of the tensor will be a sequence of tokens with a length of at most output_len+input_len.\n",
        "        \n",
        "        for b in range(batch_size):\n",
        "\n",
        "            if output_len > output_ix.shape[0]:\n",
        "                if target_output in tokenizer.decode(model_outs[b][input_len:]):\n",
        "                    done = tokenizer.decode(model_outs[b][:input_len])\n",
        "                    optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n",
        "                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n",
        "                # we decode these as tokens... is the target_output a substring?\n",
        "                # if so, we print the target_output and the decoded string that contains it\n",
        "                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n",
        "\n",
        "\n",
        "            if tokenizer.decode(model_outs[b][input_len:]) == target_output:\n",
        "                done = tokenizer.decode(model_outs[b][:input_len])\n",
        "                optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n",
        "                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n",
        "                # we decode these as tokens... is the target_output equal to output string?\n",
        "                # Nothing printed in this case.\n",
        "                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n",
        "            \n",
        "            if done is not None and rand_after:\n",
        "                input.data[b] = torch.rand_like(input[b])\n",
        "                # Random re-initialisation (if 'rand_after' set to True)\n",
        "\n",
        "  \n",
        "        if ((e+1) % w_freq == 0) or done and return_early:\n",
        "            display.clear_output(wait=True) \n",
        "            print('\\033[H\\033[J')  \n",
        "        # Every w epochs we write to log, unless we have found an optimised input before that and 'return_early' == True. \n",
        "        # I'm still not entirely sure about the idea of 'return_early'.\n",
        "\n",
        "            if plt_loss:\n",
        "                plt.plot(range(len(total_losses)), total_losses, label='Total Loss', color='black')\n",
        "                plt.plot(range(len(losses)), losses, label='Output Loss')\n",
        "                plt.plot(range(len(dists)), dists, label='Emb Dist Loss')\n",
        "                plt.plot(range(len(perps)), perps, label='Perp Loss')\n",
        "                plt.yscale('log')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "            print('Inputs found: ', optimised_inputs)\n",
        "            print('{}/{} Output Loss: {} Emb Dist Loss: {} Perp Loss: {} LR: {}'.format(e+1, epochs, batch_loss, mean_token_dist, perp_loss, optimiser.param_groups[0]['lr']))\n",
        "            if verbose == 3:\n",
        "                print('Target Probs: {}\\nTarget Logits: {}\\nInput Dists: {}\\nInput Perplexity: {}\\n'.format(target_probs.detach().cpu().numpy(), target_logits.detach().cpu().numpy(), token_dist.detach().cpu().numpy(), perp.detach().reshape(-1).cpu().numpy()))\n",
        "            # Optimised inputs and additional information are printed as part of log\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                if verbose > 0:\n",
        "                    if verbose == 2:\n",
        "                        print(b, repr(' Raw embeddings: {}'.format(''.join([closest_tokens(e)[0][0] for e in emb[b]]))))\n",
        "                        # Change name to clarify (output of model if we just put in raw embeddings)\n",
        "                        # prints batch number; closest_tokens(e)[0] is a list of tokens, closest_tokens(e)[0] is the first (closest) of these\n",
        "                        # these get joined with separator '' (SHOULDN'T THAT BE ' '?)  \n",
        "                    print(b, repr(' Closest embeddings: {}'.format(tokenizer.decode(model_outs[b]), '\\n')))\n",
        "                        # WON'T THIS give string decodings of the embeddings, rather than the embeddings themselves?\n",
        "                else:\n",
        "                    print(repr(tokenizer.decode(model_outs[b])), end=' ')\n",
        "                    # The least verbose printed output. The 'end' parameter is used to specify the end-of-line string that is appended to the output. \n",
        "                    # By default, this is a newline character, but in this case it has been set to a single space character, \n",
        "                    # so the output will be separated by spaces rather than newlines.\n",
        "\n",
        "            if done and return_early:\n",
        "                print('\\nOptimised Input: \"{}\"'.format(done))\n",
        "                results['optimised_inputs'] = optimised_inputs\n",
        "                return results\n",
        "                # we know optimised_inputs set contains a single element in this case\n",
        "            \n",
        "        optimiser.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimiser.step()\n",
        "        # I assume these three lines are standard NN optimisation stuff?\n",
        "\n",
        "        if lr_decay:\n",
        "            scheduler.step(total_loss)\n",
        "         # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5) gets used if lr_decay == True\n",
        "        done = None\n",
        "\n",
        "    results['optimised_inputs'] = optimised_inputs\n",
        "    return results\n",
        "    # that's a set of strings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to implement k-means algorithm\n",
        "def kmeans(num_clusters):\n",
        "\n",
        "    # randomly generate a centroid for each input token position, then normalise to vocab embedding span\n",
        "    centroids = torch.rand(num_clusters, word_embeddings.shape[-1]).to(device)\n",
        "    centroids = normalise(centroids,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n",
        "\n",
        "    # euclidean distance threshold to break out of loop\n",
        "    threshold = 0.01\n",
        "\n",
        "    iterations = 0\n",
        "    while True:\n",
        "        iterations += 1\n",
        "        distances = torch.cdist(word_embeddings, centroids, p=2)\n",
        "        closest_distance, closest_centroid = torch.min(distances, dim = -1)\n",
        "\n",
        "        clusters = []\n",
        "        mean_distances = []\n",
        "        for i in range(centroids.shape[0]):\n",
        "            mask = closest_centroid == i\n",
        "            clusters.append(word_embeddings[mask])\n",
        "            mean_distances.append(closest_distance[mask].mean().item())\n",
        "\n",
        "        new_centroids = []\n",
        "        for i in range(len(clusters)):\n",
        "            new_centroids.append(clusters[i].mean(dim=0))\n",
        "\n",
        "        new_centroids = torch.stack(new_centroids)\n",
        "    \n",
        "        # compute the euclidean distance between old and new centroids\n",
        "        distance = torch.norm(new_centroids - centroids, dim=-1)\n",
        "        print('iteration: ', iterations, '\\n max distance between old and new centroids:', torch.max(distance).item())\n",
        "\n",
        "        if torch.max(distance) < threshold:\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    for i in range(len(clusters)):\n",
        "      print('Cluster', i, ' contains ', len(clusters[i]), ' embeddings.')\n",
        "    for j in range(num_clusters):\n",
        "      #print('Centroid ', j, ':', centroids[j])\n",
        "      print('Closest token to centroid ', j,': ', closest_tokens(centroids[j])[0], closest_tokens(centroids[j])[1])\n",
        "    return centroids\n",
        "\n",
        "kmeans(15)"
      ],
      "metadata": {
        "id": "2jZJMakszJ_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060ad91e-fcd6-4f00-a201-e18f05ba754a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:  1 \n",
            " max distance between old and new centroids: 8.770989418029785\n",
            "iteration:  2 \n",
            " max distance between old and new centroids: 0.692197859287262\n",
            "iteration:  3 \n",
            " max distance between old and new centroids: 0.39879119396209717\n",
            "iteration:  4 \n",
            " max distance between old and new centroids: 0.3174588978290558\n",
            "iteration:  5 \n",
            " max distance between old and new centroids: 0.2868925929069519\n",
            "iteration:  6 \n",
            " max distance between old and new centroids: 0.21167714893817902\n",
            "iteration:  7 \n",
            " max distance between old and new centroids: 0.13918468356132507\n",
            "iteration:  8 \n",
            " max distance between old and new centroids: 0.12666721642017365\n",
            "iteration:  9 \n",
            " max distance between old and new centroids: 0.08788508921861649\n",
            "iteration:  10 \n",
            " max distance between old and new centroids: 0.09732615947723389\n",
            "iteration:  11 \n",
            " max distance between old and new centroids: 0.08190497010946274\n",
            "iteration:  12 \n",
            " max distance between old and new centroids: 0.06841404736042023\n",
            "iteration:  13 \n",
            " max distance between old and new centroids: 0.06614221632480621\n",
            "iteration:  14 \n",
            " max distance between old and new centroids: 0.08181934058666229\n",
            "iteration:  15 \n",
            " max distance between old and new centroids: 0.06043571978807449\n",
            "iteration:  16 \n",
            " max distance between old and new centroids: 0.06776033341884613\n",
            "iteration:  17 \n",
            " max distance between old and new centroids: 0.08860436826944351\n",
            "iteration:  18 \n",
            " max distance between old and new centroids: 0.10538746416568756\n",
            "iteration:  19 \n",
            " max distance between old and new centroids: 0.11441270262002945\n",
            "iteration:  20 \n",
            " max distance between old and new centroids: 0.12470216304063797\n",
            "iteration:  21 \n",
            " max distance between old and new centroids: 0.12688574194908142\n",
            "iteration:  22 \n",
            " max distance between old and new centroids: 0.10032492130994797\n",
            "iteration:  23 \n",
            " max distance between old and new centroids: 0.044664543122053146\n",
            "iteration:  24 \n",
            " max distance between old and new centroids: 0.03694847226142883\n",
            "iteration:  25 \n",
            " max distance between old and new centroids: 0.039338551461696625\n",
            "iteration:  26 \n",
            " max distance between old and new centroids: 0.035551998764276505\n",
            "iteration:  27 \n",
            " max distance between old and new centroids: 0.031026888638734818\n",
            "iteration:  28 \n",
            " max distance between old and new centroids: 0.02704324945807457\n",
            "iteration:  29 \n",
            " max distance between old and new centroids: 0.020459307357668877\n",
            "iteration:  30 \n",
            " max distance between old and new centroids: 0.018862439319491386\n",
            "iteration:  31 \n",
            " max distance between old and new centroids: 0.019271550700068474\n",
            "iteration:  32 \n",
            " max distance between old and new centroids: 0.018565021455287933\n",
            "iteration:  33 \n",
            " max distance between old and new centroids: 0.016306208446621895\n",
            "iteration:  34 \n",
            " max distance between old and new centroids: 0.015785448253154755\n",
            "iteration:  35 \n",
            " max distance between old and new centroids: 0.01736230030655861\n",
            "iteration:  36 \n",
            " max distance between old and new centroids: 0.019007885828614235\n",
            "iteration:  37 \n",
            " max distance between old and new centroids: 0.018145816400647163\n",
            "iteration:  38 \n",
            " max distance between old and new centroids: 0.023595016449689865\n",
            "iteration:  39 \n",
            " max distance between old and new centroids: 0.028441499918699265\n",
            "iteration:  40 \n",
            " max distance between old and new centroids: 0.039179034531116486\n",
            "iteration:  41 \n",
            " max distance between old and new centroids: 0.05552686005830765\n",
            "iteration:  42 \n",
            " max distance between old and new centroids: 0.0631984993815422\n",
            "iteration:  43 \n",
            " max distance between old and new centroids: 0.0638527199625969\n",
            "iteration:  44 \n",
            " max distance between old and new centroids: 0.040454789996147156\n",
            "iteration:  45 \n",
            " max distance between old and new centroids: 0.01652294211089611\n",
            "iteration:  46 \n",
            " max distance between old and new centroids: 0.006587841548025608\n",
            "Cluster 0  contains  4130  embeddings.\n",
            "Cluster 1  contains  2942  embeddings.\n",
            "Cluster 2  contains  2845  embeddings.\n",
            "Cluster 3  contains  3176  embeddings.\n",
            "Cluster 4  contains  3056  embeddings.\n",
            "Cluster 5  contains  2057  embeddings.\n",
            "Cluster 6  contains  2040  embeddings.\n",
            "Cluster 7  contains  4427  embeddings.\n",
            "Cluster 8  contains  2929  embeddings.\n",
            "Cluster 9  contains  2211  embeddings.\n",
            "Cluster 10  contains  3728  embeddings.\n",
            "Cluster 11  contains  1573  embeddings.\n",
            "Cluster 12  contains  3543  embeddings.\n",
            "Cluster 13  contains  6789  embeddings.\n",
            "Cluster 14  contains  4811  embeddings.\n",
            "Closest token to centroid  0 :  ['In'] tensor([818], device='cuda:0')\n",
            "Closest token to centroid  1 :  ['�'] tensor([182], device='cuda:0')\n",
            "Closest token to centroid  2 :  ['\\x19'] tensor([213], device='cuda:0')\n",
            "Closest token to centroid  3 :  ['\\x07'] tensor([195], device='cuda:0')\n",
            "Closest token to centroid  4 :  [' externalToEVA'] tensor([30212], device='cuda:0')\n",
            "Closest token to centroid  5 :  [' externalToEVA'] tensor([30212], device='cuda:0')\n",
            "Closest token to centroid  6 :  ['�'] tensor([187], device='cuda:0')\n",
            "Closest token to centroid  7 :  ['\\x07'] tensor([195], device='cuda:0')\n",
            "Closest token to centroid  8 :  [' externalToEVA'] tensor([30212], device='cuda:0')\n",
            "Closest token to centroid  9 :  ['\\x1c'] tensor([216], device='cuda:0')\n",
            "Closest token to centroid  10 :  [' externalToEVA'] tensor([30212], device='cuda:0')\n",
            "Closest token to centroid  11 :  ['�'] tensor([187], device='cuda:0')\n",
            "Closest token to centroid  12 :  ['�'] tensor([187], device='cuda:0')\n",
            "Closest token to centroid  13 :  [' externalToEVA'] tensor([30212], device='cuda:0')\n",
            "Closest token to centroid  14 :  ['�'] tensor([182], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0096, -0.0378,  0.0924,  ...,  0.0069, -0.0050, -0.0009],\n",
              "        [ 0.0300, -0.0511,  0.1464,  ...,  0.0149,  0.0324,  0.0033],\n",
              "        [-0.0413, -0.0685,  0.1154,  ...,  0.0044, -0.0066,  0.0699],\n",
              "        ...,\n",
              "        [ 0.0080, -0.0443,  0.1602,  ...,  0.0484,  0.0010,  0.0235],\n",
              "        [ 0.0074, -0.0788,  0.0856,  ...,  0.0294, -0.0034,  0.0391],\n",
              "        [-0.0015, -0.0723,  0.1938,  ...,  0.0301, -0.0018,  0.0412]],\n",
              "       device='cuda:0', grad_fn=<StackBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tev0cDbkdbxO"
      },
      "outputs": [],
      "source": [
        "ix = tokenizer.encode(\"\")\n",
        "# list of 'vocab indices'\n",
        "print(ix)\n",
        "print([tokenizer.decode(i) for i in ix])\n",
        "# prints reconstruction of input string\n",
        "print(len(ix))\n",
        "# prints number of tokens\n",
        "output_len=2\n",
        "model_out = model.generate(torch.tensor(ix).unsqueeze(0).to(device), max_length = output_len + len(ix))\n",
        "print(tokenizer.decode(model_out[0]))\n",
        "# pushes input string throught GPT2 (or whichever model) iteratively producing output_len number of tokens, then prints input + output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiments = [{'base_input': False, \n",
        "                'plt_loss': False, \n",
        "                'verbose': 1, \n",
        "                'epochs': 1000, \n",
        "                'lr_decay': False, \n",
        "                'return_early': False, \n",
        "                'lr': 0.1, \n",
        "                'batch_size': 20, \n",
        "                'target_output': ' a lot of data', \n",
        "                'output_len': 4, \n",
        "                'input_len': 3, \n",
        "                'w_freq': 20, \n",
        "                'dist_reg': 1, \n",
        "                'perp_reg': 0, \n",
        "                'loss_type': 'log_prob_loss',\n",
        "                'note':''}\n",
        "                ]\n",
        "\n",
        "\n",
        "experiment_log = {}"
      ],
      "metadata": {
        "id": "y8NBHj-uOnDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for e in experiments:\n",
        "    tick = time()\n",
        "    results = optimise_input(**e)\n",
        "    tock = time()\n",
        "    rt = tock - tick\n",
        "    results.update({'runtime':rt})\n",
        "    \n",
        "    with open(\"backwards_results.json\",\"a\") as f:\n",
        "        f.write(json.dumps({tick:results}))\n"
      ],
      "metadata": {
        "id": "81r4gT4m2NQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}