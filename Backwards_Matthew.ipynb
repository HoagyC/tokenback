{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessicamarycooper/Backwards/blob/main/Backwards_Matthew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ],
      "metadata": {
        "id": "MOgggCvPZoWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ar1aNn7TxI8r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "a13539aedc89401c8dc95605ff95c5a3",
            "08bf66ccf8c74c6ab7101385415c7b41",
            "23d9ec6919554b04b97d8ae95eadf7ec",
            "ee6da364c2fd4f119a10ac77a88ca011",
            "15997e2e26c04fb387f22b7a05669efa",
            "fba2ee9137ad4737ae5ec69d90e9554c",
            "c62c9d74e387404cbc9e3d9a3b0ccc76",
            "b5c3dd3dec744738a10f3ae84c42df2f",
            "e4e37d9b6bf2499481a5ac9d74ef4a61",
            "ec76f2ac7a384a33860b91b9b2024770",
            "dfc992bded464758b040a6595e2c38a1",
            "dac6c91243664d2b8aada33b858edcd5",
            "e070ab3366e648999076851f1680725b",
            "ecc6e036195f4178bfd6c8a2f84a92e5",
            "99c978a555224bab98fc4602a137abd6",
            "b4c96f32ac654f6fab35617265b8f9c4",
            "69f6b33ca4fa483d86cec40d0204e08e",
            "b6370c98eaff4c409c3978a6ce82dc41",
            "4947ff193a4c406ca7421cba5970e1d3",
            "035f221ec4a643b4b8966b0ae22bdab1",
            "e9b4bf7b3659465a8354887e0500dc44",
            "11f90c89e6604b5da2b318c9b5783724",
            "f893f2bece43415899c86d91fe0b00b5",
            "bc96688312a740568f44c7ceeea3312e",
            "ff67053d0dcd4ab8a0cd39928d6d112e",
            "92e7ed3e02df4281a7b3de1d70ab4d9a",
            "bbf523abfa3f492dbb5214ba94e36976",
            "0dc08c23b1304fdba26d6a25aaabb9e0",
            "afb014b62cbe4565840b9f8748cd32fe",
            "1198557a758f422d8a02a708f612648a",
            "93eec23f1b8d4f99930d45bdb8102690",
            "aaaa5f07ffe2433084f6c4a4079aa54d",
            "fd003c97c2d740ce8cd6c1eb88c6e957",
            "8b53c8f460d34173bead54f41c97326a",
            "9a886d06281a4d6bba1f756aa98f6f8b",
            "9865bd1bb2b0438484b3cb6e2a592ddd",
            "8de662f8f14a4920bee9322bf8cc9f94",
            "c6381b0a8ed348c685e884fbbe9dfc9b",
            "47803c89c20c4f76a11988a963d541a9",
            "72f06c169cd0485980cd2b66ecae94ab",
            "ed96e266a52f46f0a8bf68bce6abd491",
            "f9c8382ab1e24028b7c53a7f15a5deb1",
            "5539bc849e9946f6aad5ac03c6be3ed3",
            "7e404ba8a0414f52bc062d026e996ea4"
          ]
        },
        "outputId": "01ded3c9-7228-4eca-d0a0-f93faefe0a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting update\n",
            "  Downloading update-0.0.1-py2.py3-none-any.whl (2.9 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting style==1.1.0\n",
            "  Downloading style-1.1.0-py2.py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, style, update, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 style-1.1.0 tokenizers-0.13.2 transformers-4.25.1 update-0.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a13539aedc89401c8dc95605ff95c5a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dac6c91243664d2b8aada33b858edcd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f893f2bece43415899c86d91fe0b00b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b53c8f460d34173bead54f41c97326a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install update transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, utils\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import json\n",
        "utils.logging.set_verbosity_error()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vocab_len= 50257\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left')\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id, vocab_size=vocab_len).to(device)\n",
        "model.eval()\n",
        "# the model will be in evaluation, not training, mode throughout\n",
        "word_embeddings = model.transformer.wte.weight.to(device)   \n",
        "# 'word_embeddings' tensor gives emeddings for each token in the vocab for this model,\n",
        "# has shape (vocab_len, embedding_dimension) which in this case = (50257, 768)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qCgMUuO_33Wp"
      },
      "outputs": [],
      "source": [
        "def normalise(x, min_max=[]):     \n",
        "# normalises values of (array or tensor) x according to first (min) and second (max) values in list min_max. \n",
        "# This effectively defaults to [0,1] if the list doesn't contain exactly two elements. \n",
        "# The original code threw an error if min_max had length 1, so it's been changed slightly.\n",
        "\n",
        "# First normalise x to [0,1]\n",
        "    rnge = x.max() - x.min()\n",
        "    if rnge > 0:\n",
        "        x = (x - x.min())/rnge\n",
        "\n",
        "# Now, if there's a min and max given in min_max list, multiply by difference and add minimum\n",
        "    if len(min_max) > 1:\n",
        "        rnge = min_max[1] - min_max[0]\n",
        "        x = x * rnge + min_max[0]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def closest_tokens(emb, n=1):      \n",
        "# This finds the n tokens in the vocabulary that are closest in the embedding space (in terms of Euclidean distance) to a given word embedding (‘emb’).\n",
        "# Note that here 'emb' may or may not correspond to a token (i.e., it may or may not be a 'legal' embedding).\n",
        "# Function returns a 4-tuple (list of the n tokens, list of their indices, list of their distances from emb, and list of their embedding vectors)\n",
        "    torch.cuda.empty_cache()\n",
        "    dists = torch.linalg.norm(word_embeddings - emb, dim=1)\n",
        "    sorted_dists, ix = torch.sort(dists)\t \n",
        "    # sorted_dists is a list of all embedding distances from 'emb', across entire vocab, sorted in increasing order, \n",
        "    # ix is a list of their corresponding 'vocab indices'\n",
        "    tokens = [tokenizer.decode(i) for i in ix[:n]]\n",
        "    # For each of the first n 'vocab indices' in ix, we decode it into the string version of the corresponding token. \n",
        "    # These strings then constitute the list 'tokens'.\n",
        "    ixs = ix[:n]\n",
        "    dists = sorted_dists[:n]\n",
        "    embs = word_embeddings[ixs]  # Each of these n 'embeddings' is a tensor of shape (768,)\n",
        "    return tokens, ixs, dists, embs  \n",
        "\n",
        "\n",
        "def model_emb(inputs_embeds, output_len):\n",
        "# 'input_embeds' is a tensor of shape (batch_size, input_len, embedding_dim)\n",
        "# 'output_len' is an integer specifying the number of output tokens to generate\n",
        "# Note that this function doesn't involve a target output. It simply takes a tensor of input embeddings (based on input length),\n",
        "# calculates perplexities for that batch of input sequences,\n",
        "# and runs the batch of input sequences through GPT2, for each finding next tokens iteratively 'output_len' number of times\n",
        "    embs = inputs_embeds   # This is going to get expanded using 'output_embs'\n",
        "    logits = []\n",
        "    ixs = []\n",
        "    input_logits = None\n",
        "    for i in range(output_len):\n",
        "        model_out = model(inputs_embeds=embs, return_dict=True)\n",
        "        # Does a forward pass of GPT2 (or whichever model) on a batch of inputs (given as a tensor 'embs' of embeddings).\n",
        "        # This 'embs' will expand along its 1st dimension with each iteration.\n",
        "        # Outputs logits and more (hidden states, attention, etc.) as a dictionary 'model_out'.\n",
        "        # But we'll only be concerned with model_out.logits.\n",
        "\n",
        "        if i == 0:\n",
        "            input_logits = model_out.logits \n",
        "            # On first pass through loop, we simply use the logits of the model output\n",
        "            # That's a tensor of shape (batch_size, input_len, vocab_size) giving logits for each input in each batch.\n",
        "            # Presumably for each input, this is conditioned on the inputs that preceded it?\n",
        "\n",
        "        # On every pass throught the loop (including the first), we defined this tensor of shape (batch_size, 1, vocab_size):\n",
        "        last_logits = model_out.logits[:,-1].unsqueeze(1)  \n",
        "        # model_out.logits[:,-1] will be a 2D tensor of shape (batch_size, vocab_size), just giving logits for last input/embedding across all batches/tokens\n",
        "        # unsqueezing, we get tensor of shape (batch_size, 1, vocab_size) also giving logits of last input/embedding, differently formatted  \n",
        "        logits.append(last_logits)  # appends last_logits tensor to the 'logits' list \n",
        "        ix = torch.argmax(last_logits, dim=-1)  # for each batch, finds the vocab index of the token with the largest logit in last_logits\n",
        "        ixs.append(ix) # ...and appends this tensor of shape (batch_size,) (containing indices) it to the list 'ixs'\n",
        "        output_embs = word_embeddings[ix]   # for each batch, finds embedding for the token with that index...\n",
        "        embs = torch.cat([embs, output_embs], dim=1)  #...concatenates that tensor of embeddings to the 'embs' tensor in the first dimension before next iteration\n",
        "\n",
        "     # When the loop is completed 'embs' will be a tensor containing all of the input and output word embeddings produced by the model   \n",
        "     # ...so presumably of shape (batch_size, input_len + output_len, embedding_dim)\n",
        "\n",
        "    logits = torch.cat(logits, dim=1)   # this converts logits from a list of tensors to a single tensor, by concatenating all of the tensors in the list\n",
        "                                        # it will have shape (batch_size, output_len, vocab_size)\n",
        "    perp = perplexity(input_logits)     # 'input_logits' was calculated on first pass through loop where only input embeddings were involved\n",
        "    return logits, embs, perp          \n",
        "    # logits has shape (batch_size, output_len, vocab_size),         CHECK THAT!\n",
        "    # embs has shape (batch_size, input_len + output_len, embedding_dim)\n",
        "    # perp has shape (batch_size,)\n",
        "\n",
        "\n",
        "def perplexity(logits):\n",
        "    # logits is of shape (batch_size, 'sequence length', vocab_size)\n",
        "    # for all current calls, 'sequence length' is going to be input_len\n",
        "    probs, ix = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n",
        "    # torch.softmax(logits, dim=-1) will also be a tensor of shape (batch_size, 'sequence length', vocab_size), \n",
        "    # but where the logits in the last dimension get converted into probabilities via softmax. torch.max() then pull out the largest of these and its index\n",
        "    # probs is a tensor that contains the maximum probability for each token in the embedding sequence, shape (batch_size, 'sequence length')\n",
        "    # ix is a tensor that contains the corresponding indices, also with shape (batch_size, 'sequence length')\n",
        "    perp = 1/ (torch.prod(probs, dim=-1)**(1/probs.shape[-1])) - 1\n",
        "    # defines a scalar that's larger with greater uncertainty (so if the probs are small, their product is small, the reciprocal of some power is large)\n",
        "    # probs.shape[-1] is output_len; the idea of raising the probs product to power 1/output_len is to make perplexities comparable across different output lengths\n",
        "    return perp\n",
        "\n",
        "\n",
        "# Here's the key function that optimises for a sequence of input embeddings, given a target_output string:\n",
        "def optimise_input(epochs=100, \n",
        "                   lr=0.1, \n",
        "                   rand_after=False,    # Do we re-initialise inputs tensor with random entries when an optimal input is found?\n",
        "                   w_freq=10,           # logging (write) frequency\n",
        "                   base_input=False,      # If False, start_inputs will be entirely random\n",
        "                   batch_size=1, \n",
        "                   input_len=1, \n",
        "                   target_output=tokenizer.eos_token,    # Default target output is the \"end-of-string\" token; this won't generally be used\n",
        "                   output_len=None,\n",
        "                   dist_reg=1,       # distance regularisation coefficient\n",
        "                   perp_reg=0,       # perplexity regularisation coefficient; setting to 0 means perplexity loss isn't a thing\n",
        "                   plt_loss=False,   # Do we plot loss?\n",
        "                   loss_type='log_prob_loss', \n",
        "                   seed=0,\n",
        "                   return_early=True,    # finishes if single optimised input is found\n",
        "                   verbose=0,            # Controls how much info gets logged.\n",
        "                   lr_decay=False,       # Use learning rate decay? If so, a scheduler gets invoked.\n",
        "                   noise_coeff = 0.01, **kwargs):     # Introduced for generality in the construction of start_input[1:] below.\n",
        "    \n",
        "    torch.manual_seed(seed)               # sets up PyTorch random number generator\n",
        "\n",
        "    results = {'args': locals()}\n",
        "\n",
        "    if plt_loss:\n",
        "        plt.rcParams.update({'figure.figsize': (40,6)})\n",
        "\n",
        "    total_losses = []\n",
        "    losses = []\n",
        "    dists = []\n",
        "    perps = []\n",
        "    optimised_inputs = dict()\n",
        "    done = None\n",
        "\n",
        "    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n",
        "    # output_ix is a 1-D tensor of shape (output_len,) that contains the indices of the tokens in the encoding of the string 'target_output'\n",
        "    # tokenizer.encode(target_output, return_tensors='pt') is a list containing this one tensor, hence the need for the [0]\n",
        "    # \"return_tensors='pt'\" ensures that we get a tensor in PyTorch format\n",
        "\n",
        "    if output_len == None or output_len < output_ix.shape[0]:                    # This won't generally be the case, but if we don't specify output_len (i.e. it's == None), then...\n",
        "        output_len = output_ix.shape[0]       # ...it will be set to the number of tokens in the encoding of the string 'target_output'\n",
        "    # Why not just set output_len = output_ix.shape[0] in all cases?\n",
        "    # Will there be situations where we want output_len to be of a different size to the number of tokens in target_output?\n",
        "\n",
        "    print('Optimising input of length {} to maximise output logits for \"{}\"'.format(input_len, target_output))\n",
        "    # Typically this would print something like 'Optimising input of length 6 to maximise output logits for \"KILL ALL HUMANS!\"'.\n",
        "\n",
        "    if base_input == False:\n",
        "        start_input = torch.rand(batch_size, input_len, word_embeddings.shape[-1]).to(device)\n",
        "        # If no base_input is provided, we construct start_input as a random tensor \n",
        "        # of shape (batch_size, input_len, embedding_dim) (embedding_dim = 768 for this GPT-2 model).\n",
        "        start_input = normalise(start_input,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n",
        "        # We normalise this random tensor so that its minimum and maximum values correspond to those in the entire word_embeddings tensor\n",
        "        # This dispenses with whole swathes of \"input space\" which contain no legal token embeddings \n",
        "        # (we're limiting ourselves to a kind of \"hull\" defined by the 50527 vocab tokens in the embedding space), \n",
        "        # which is a sensible place to look for optimised inputs.\n",
        "    else:\n",
        "        start_input = word_embeddings[output_ix].mean(dim=0).repeat(batch_size, input_len, 1)\n",
        "\n",
        "        if batch_size > 1:\n",
        "            start_input[1:] += (torch.rand_like(start_input[1:]) + torch.full_like(start_input[1:], -0.5)) * noise_coeff\n",
        "        #...and if we have more than one element in our batch, we \"noise\" the rest. \n",
        "        # This was originally done using \"*=\" (multiplying entries by small random numbers)\n",
        "        # We've changed this to \"+=\" (adding  small random numbers instead of multiplying by them).\n",
        "        # The original code would have pushed everything in a positive direction, hence the use of a tensor full of -0.5's.       \n",
        "\n",
        "    \n",
        "    input = torch.nn.Parameter(start_input, requires_grad=True)\n",
        "    # input is not a tensor, it's a Parameter object that wraps a tensor and adds additional functionality. \n",
        "    # 'input.data' is used below\n",
        "    \n",
        "    optimiser = torch.optim.Adam([input], lr=lr)\n",
        "    # standard optimiser; note that it generally operates on a list of tensors, so we're giving it a list of one tensor; standard learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5)\n",
        "    # this is used when loss hasn't improved for 20 timesteps; this scheduler will reduce the lr by a 'factor' of 0.5 when the \n",
        "    # validation loss stops improving for 'patience' (here 20) epochs, and will wait 'cooldown' (here 20) epochs before resuming normal operation.\n",
        "\n",
        "    # now we loop across training epochs\n",
        "    for e in range(epochs):\n",
        "\n",
        "        logits, emb, perp = model_emb(torch.clamp(input, word_embeddings.min(), word_embeddings.max()), output_len)\n",
        "        # Does forward pass on a 'clamped' version of the 'input' tensor (done to contain it within the 'hull' of the vocabulary within 'input space').\n",
        "        # Iterates to produce an output of output_len tokens, \n",
        "        # returns: 'logits' = tensor of logits for output, of shape (batch_size, output_len, vocab_size)\n",
        "        # 'emb': tensor of embeddings for input+output of shape (batch_size, input_len + output_len, embedding_dim); \n",
        "        # 'perp': the input sequence perplexities tensor, of shape (batch_size,)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        # For each batch, output, converts the sequence of logits (of length 'vocab_size') in the 'logits' tensor to probabilities, using softmax\n",
        "\n",
        "        logits = (logits - logits.min(dim=-1)[0].unsqueeze(-1)) / (logits.max(dim=-1)[0].unsqueeze(-1) - logits.min(dim=-1)[0].unsqueeze(-1))\n",
        "        # This appears to be normalising the logits for each batch/output embedding so they're all between 0 and 1... \n",
        "        # This is for ease of visualisation.\n",
        "\n",
        "        perp_loss = perp.mean() * perp_reg\n",
        "        # That's taking the mean perp value across all batches, then regularising it. Currently perp_reg is set to 0, so perp_loss = 0.\n",
        "\n",
        "        if output_len > output_ix.shape[0]:\n",
        "            target_logits = torch.stack([logits[:, :, ix] for ix in output_ix], dim=-1)\n",
        "            target_logits = torch.max(target_logits, dim=-1)[0]\n",
        "            # logits is shape (batch_size, output_len, vocab_size) \n",
        "            # We throw out everything in the final dimension except those logits corresponding to indices of tokens in the target_ouput\n",
        "            # This gives tensor with shape (batch_size, output_len, output_ix.shape[0])\n",
        "            # We then take the maximum of those for each batch, output; this gives shape (batch_size, output_len)\n",
        "            # The [0] returns just the max (torch.max returns (max, indices) tuple)\n",
        "            target_probs = torch.stack([probs[:, :, ix] for ix in output_ix], dim=-1)\n",
        "            target_probs = torch.max(target_probs, dim=-1)[0]\n",
        "            # This does the analogous thing for probs.\n",
        "\n",
        "        else:\n",
        "            target_logits = torch.stack([logits[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n",
        "            target_probs = torch.stack([probs[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n",
        "            # This handles case where output_len == output_ix.shape[0]\n",
        "            # target_logits now of shape (batch_size, output_len)?\n",
        "            # output_len < output_ix.shape[0] was dealt with in line 135\n",
        "            \n",
        "        token_dist = torch.stack([torch.stack([closest_tokens(e)[2].squeeze(-1) for e in input[b]]) for b in range(batch_size)])\n",
        "        # As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\n",
        "        # legal token embedding for each input embedding in each batch\n",
        "        mean_token_dist = token_dist.mean() * dist_reg\n",
        "        # A single scalar value, taking mean across the batch and input embeddings? \n",
        "\n",
        "\n",
        "        # There are currently four loss types, many more could be introduced.\n",
        "        # log_prob_loss is the current default.\n",
        "        if loss_type == 'logit_loss':\n",
        "            loss = 1-target_logits\n",
        "        elif loss_type == 'log_prob_loss':\n",
        "            loss = -torch.log(target_probs)\n",
        "        elif loss_type == 'prob_loss':\n",
        "            loss = 1-target_probs\n",
        "        elif loss_type == 'CE':\n",
        "            loss = torch.nn.functional.cross_entropy(logits.swapaxes(-1,-2), output_ix.repeat(batch_size, 1), reduction=None)\n",
        "        else:\n",
        "            print(loss_type + 'is not implemented.')\n",
        "            return \n",
        "\n",
        "        batch_loss = loss.mean()\n",
        "\n",
        "        total_loss = torch.stack([mean_token_dist, batch_loss, perp_loss]).mean()\n",
        "        # This is this just (mean_token_dist + loss + perp_loss)/3 tensorised across batches, yes?\n",
        "\n",
        "        total_losses.append(total_loss.detach().cpu().data)\n",
        "        losses.append(batch_loss.detach().cpu().data)\n",
        "        dists.append(mean_token_dist.detach().cpu().data)\n",
        "        perps.append(perp_loss.detach().cpu().data)\n",
        "        # these four lists were intialised above. We're appeneding to the list each epoch. All are scalars.\n",
        "\n",
        "        closest_ix = torch.stack([torch.stack([closest_tokens(e)[1] for e in b]) for b in input]).squeeze(-1)\n",
        "        # This is similar to above, but building a tensor of indices of nearest embeddings, rather than distances.\n",
        "        # Iterates over batches, and for each batch iterates over embeddings, giving tensor of shape (batch_size, input_len).\n",
        "\n",
        "        model_outs = model.generate(closest_ix, max_length = output_len+input_len)\n",
        "        # The 'closest_ix' tensor is passed as the initial input sequence to the model, \n",
        "        # and the max_length parameter specifies the maximum length of the total sequence to generate.\n",
        "        # The output sequence will be terminated either when the end-of-sequence token is generated \n",
        "        # or when the maximum length is reached, whichever occurs first.\n",
        "        # \n",
        "        # The output of the model.generate method will be a tuple containing the generated sequences and the model's internal states. \n",
        "        # The generated sequences will be stored in a tensor of shape (batch_size, output_len+input_len). \n",
        "        # Each element of the tensor will be a sequence of tokens with a length of at most output_len+input_len.\n",
        "        \n",
        "        for b in range(batch_size):\n",
        "\n",
        "            if output_len > output_ix.shape[0]:\n",
        "                if target_output in tokenizer.decode(model_outs[b][input_len:]):\n",
        "                    done = tokenizer.decode(model_outs[b][:input_len])\n",
        "                    optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n",
        "                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n",
        "                # we decode these as tokens... is the target_output a substring?\n",
        "                # if so, we print the target_output and the decoded string that contains it\n",
        "                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n",
        "\n",
        "\n",
        "            if tokenizer.decode(model_outs[b][input_len:]) == target_output:\n",
        "                done = tokenizer.decode(model_outs[b][:input_len])\n",
        "                optimised_inputs.update({done:loss[b].detach().cpu().numpy().tolist()})\n",
        "                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n",
        "                # we decode these as tokens... is the target_output equal to output string?\n",
        "                # Nothing printed in this case.\n",
        "                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n",
        "            \n",
        "            if done is not None and rand_after:\n",
        "                input.data[b] = torch.rand_like(input[b])\n",
        "                # Random re-initialisation (if 'rand_after' set to True)\n",
        "\n",
        "  \n",
        "        if ((e+1) % w_freq == 0) or done and return_early:\n",
        "            display.clear_output(wait=True) \n",
        "            print('\\033[H\\033[J')  \n",
        "        # Every w epochs we write to log, unless we have found an optimised input before that and 'return_early' == True. \n",
        "        # I'm still not entirely sure about the idea of 'return_early'.\n",
        "\n",
        "            if plt_loss:\n",
        "                plt.plot(range(len(total_losses)), total_losses, label='Total Loss', color='black')\n",
        "                plt.plot(range(len(losses)), losses, label='Output Loss')\n",
        "                plt.plot(range(len(dists)), dists, label='Emb Dist Loss')\n",
        "                plt.plot(range(len(perps)), perps, label='Perp Loss')\n",
        "                plt.yscale('log')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "            print('Inputs found: ', optimised_inputs)\n",
        "            print('{}/{} Output Loss: {} Emb Dist Loss: {} Perp Loss: {} LR: {}'.format(e+1, epochs, batch_loss, mean_token_dist, perp_loss, optimiser.param_groups[0]['lr']))\n",
        "            if verbose == 3:\n",
        "                print('Target Probs: {}\\nTarget Logits: {}\\nInput Dists: {}\\nInput Perplexity: {}\\n'.format(target_probs.detach().cpu().numpy(), target_logits.detach().cpu().numpy(), token_dist.detach().cpu().numpy(), perp.detach().reshape(-1).cpu().numpy()))\n",
        "            # Optimised inputs and additional information are printed as part of log\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                if verbose > 0:\n",
        "                    if verbose == 2:\n",
        "                        print(b, repr(' Raw embeddings: {}'.format(''.join([closest_tokens(e)[0][0] for e in emb[b]]))))\n",
        "                        # Change name to clarify (output of model if we just put in raw embeddings)\n",
        "                        # prints batch number; closest_tokens(e)[0] is a list of tokens, closest_tokens(e)[0] is the first (closest) of these\n",
        "                        # these get joined with separator '' (SHOULDN'T THAT BE ' '?)  \n",
        "                    print(b, repr(' Closest embeddings: {}'.format(tokenizer.decode(model_outs[b]), '\\n')))\n",
        "                        # WON'T THIS give string decodings of the embeddings, rather than the embeddings themselves?\n",
        "                else:\n",
        "                    print(repr(tokenizer.decode(model_outs[b])), end=' ')\n",
        "                    # The least verbose printed output. The 'end' parameter is used to specify the end-of-line string that is appended to the output. \n",
        "                    # By default, this is a newline character, but in this case it has been set to a single space character, \n",
        "                    # so the output will be separated by spaces rather than newlines.\n",
        "\n",
        "            if done and return_early:\n",
        "                print('\\nOptimised Input: \"{}\"'.format(done))\n",
        "                results['optimised_inputs'] = optimised_inputs\n",
        "                return results\n",
        "                # we know optimised_inputs set contains a single element in this case\n",
        "            \n",
        "        optimiser.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimiser.step()\n",
        "        # I assume these three lines are standard NN optimisation stuff?\n",
        "\n",
        "        if lr_decay:\n",
        "            scheduler.step(total_loss)\n",
        "         # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5) gets used if lr_decay == True\n",
        "        done = None\n",
        "\n",
        "    results['optimised_inputs'] = optimised_inputs\n",
        "    return results\n",
        "    # that's a set of strings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to implement k-means algorithm\n",
        "\n",
        "def kmeans(num_clusters):\n",
        "\n",
        "    empty_cluster = True\n",
        "\n",
        "    # euclidean distance threshold to break out of loop\n",
        "    threshold = 0.01\n",
        "\n",
        "    while empty_cluster == True: \n",
        "        # randomly generate num_clusters centroids, stack these into a tensor of shape (num_clusters, 768)\n",
        "        # then normalise to vocab embedding span\n",
        "        centroids = torch.rand(num_clusters, word_embeddings.shape[-1]).to(device)\n",
        "        centroids = normalise(centroids,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n",
        "\n",
        "        distances = torch.cdist(word_embeddings, centroids, p=2)\n",
        "        # This will be of shape (vocab_len, num_clusters), recording distances of token embeddings from each of the centroid.\n",
        "        closest_distance, closest_centroid = torch.min(distances, dim = -1)\n",
        "        # These will be of shape (vocab_len,), recording the distance of each token embedding to nearest centroid, and the index of that centroid.\n",
        "\n",
        "        clusters = []\n",
        "        mean_distances = []\n",
        "        # We iterate over the centroids\n",
        "        for i in range(centroids.shape[0]):\n",
        "            mask = closest_centroid == i  # This builds a Boolean mask over the complete set of tokens, True if a token's nearest centroid is the ith.\n",
        "            clusters.append(word_embeddings[mask]) \n",
        "            # word_embeddings[mask] is a subtensor of the (50257, 768) shape tensor involving only tokens nearest ith centroid.\n",
        "            # This subtensor gets appended to the list 'clusters', which has num_clusters elements.\n",
        "\n",
        "        shapes = [clusters[i].shape[0] for i in range(len(clusters))]  \n",
        "        if 0 not in shapes:\n",
        "            empty_cluster = False \n",
        "\n",
        "    # Now we have a set on num_clusters non-empty clusters, we begin iterating centroid positions:\n",
        "    centroids_stable = False\n",
        "    iterations = 0\n",
        "\n",
        "    while centroids_stable == False:\n",
        "        iterations += 1\n",
        "        distances = torch.cdist(word_embeddings, centroids, p=2)\n",
        "        # This will be of shape (vocab_len, num_clusters), recording distances of token embeddings from each of the centroid.\n",
        "        closest_distance, closest_centroid = torch.min(distances, dim = -1)\n",
        "        # These will be of shape (vocab_len,), recording the distance of each token embedding to nearest centroid, and the index of that centroid.\n",
        "\n",
        "        clusters = []\n",
        "        mean_distances = []\n",
        "        # We iterate over the centroids\n",
        "        for i in range(centroids.shape[0]):\n",
        "            mask = closest_centroid == i  # This builds a Boolean mask over the complete set of tokens, True if a token's nearest centroid is the ith.\n",
        "            clusters.append(word_embeddings[mask]) \n",
        "            # word_embeddings[mask] is a subtensor of the (50257, 768) shape tensor involving only tokens nearest ith centroid.\n",
        "            # This subtensor gets appended to the list 'clusters' (num_clusters elements).\n",
        "            mean_distances.append(closest_distance[mask].mean().item()) \n",
        "            # closest_distance[mask] is a subtensor of the (50257,) shape tensor involving only tokens nearest ith centroid.\n",
        "            # This subtensor gets appended to the list 'mean_distances' (num_clusters elements).\n",
        "\n",
        "        new_centroids = []\n",
        "        for i in range(num_clusters):\n",
        "            new_centroids.append(clusters[i].mean(dim=0))\n",
        "            # clusters[i].mean(dim=0) is the centroid of the set of vectors encoded in the tensor clusters[i]\n",
        "            # these all get put in a list...\n",
        "\n",
        "        new_centroids = torch.stack(new_centroids)\n",
        "        # ... and the list gets stacked into a tensor of shape (num_clusters, 768)\n",
        "\n",
        "        # We now compute the euclidean distance between old and new centroids\n",
        "        distance = torch.norm(new_centroids - centroids, dim=-1)\n",
        "        #print('iteration: ', iterations, '\\n max distance between old and new centroids:', torch.max(distance).item())\n",
        "        # This is returning a \"nan\" for all iterations in some cases, usually when num_clusters > 10\n",
        "        # I can't figure out why this is happening, but possibly some error handling could just take us back \n",
        "        # to the random initialisation of the 'centroids' tensor.\n",
        "\n",
        "        if torch.max(distance) < threshold:\n",
        "            centroids_stable == True\n",
        "            break\n",
        "        centroids = new_centroids # if we're still outside the distance threshold, keep iterating\n",
        "    \n",
        " \n",
        "    #for i in range(len(clusters)):\n",
        "        #print('Cluster', i, ' contains ', len(clusters[i]), ' embeddings.')\n",
        "    #for j in range(num_clusters):\n",
        "        #print('Closest token to centroid ', j,': ', closest_tokens(centroids[j])[0], closest_tokens(centroids[j])[1].item())\n",
        "    return centroids"
      ],
      "metadata": {
        "id": "2jZJMakszJ_U"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This runs 200 batches of 50 and keeps track of the most common closest tokens to centroid embeddings, and how many appeareances they make\n",
        "token_counts = torch.zeros(vocab_len)\n",
        "for j in range(200):\n",
        "    print('batch', j)\n",
        "    centroids = kmeans(50)\n",
        "    for i in range(50):\n",
        "        token_counts[closest_tokens(centroids[i])[1].item()] +=1\n",
        "\n",
        "values, indices = torch.sort(token_counts, descending=True)\n",
        "print(indices[:50], values[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "442OukV-eUID",
        "outputId": "1fcd6eae-2f2f-4d15-b3d9-beda17bd2a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0\n",
            "batch 1\n",
            "batch 2\n",
            "batch 3\n",
            "batch 4\n",
            "batch 5\n",
            "batch 6\n",
            "batch 7\n",
            "batch 8\n",
            "batch 9\n",
            "batch 10\n",
            "batch 11\n",
            "batch 12\n",
            "batch 13\n",
            "batch 14\n",
            "batch 15\n",
            "batch 16\n",
            "batch 17\n",
            "batch 18\n",
            "batch 19\n",
            "batch 20\n",
            "batch 21\n",
            "batch 22\n",
            "batch 23\n",
            "batch 24\n",
            "batch 25\n",
            "batch 26\n",
            "batch 27\n",
            "batch 28\n",
            "batch 29\n",
            "batch 30\n",
            "batch 31\n",
            "batch 32\n",
            "batch 33\n",
            "batch 34\n",
            "batch 35\n",
            "batch 36\n",
            "batch 37\n",
            "batch 38\n",
            "batch 39\n",
            "batch 40\n",
            "batch 41\n",
            "batch 42\n",
            "batch 43\n",
            "batch 44\n",
            "batch 45\n",
            "batch 46\n",
            "batch 47\n",
            "batch 48\n",
            "batch 49\n",
            "batch 50\n",
            "batch 51\n",
            "batch 52\n",
            "batch 53\n",
            "batch 54\n",
            "batch 55\n",
            "batch 56\n",
            "batch 57\n",
            "batch 58\n",
            "batch 59\n",
            "batch 60\n",
            "batch 61\n",
            "batch 62\n",
            "batch 63\n",
            "batch 64\n",
            "batch 65\n",
            "batch 66\n",
            "batch 67\n",
            "batch 68\n",
            "batch 69\n",
            "batch 70\n",
            "batch 71\n",
            "batch 72\n",
            "batch 73\n",
            "batch 74\n",
            "batch 75\n",
            "batch 76\n",
            "batch 77\n",
            "batch 78\n",
            "batch 79\n",
            "batch 80\n",
            "batch 81\n",
            "batch 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_tokens_idxs = [30212, 187, 195, 216, 182, 179, 213, 39820, 124, 199,\n",
        "208, 125, 23090, 554, 30208, 3607, 281, 7003, 37528, 15524,\n",
        "192, 42089,  217, 39752, 183, 210, 201, 209, 190, 287,\n",
        "189, 211, 818, 206, 212, 2890, 1315, 203, 207, 14827,\n",
        "30898, 218, 219, 28124, 215, 15563, 36173, 3179, 4035, 2670]\n",
        "most_common_tokens = [tokenizer.decode(i) for i in most_common_tokens_idxs]\n",
        "print(most_common_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI9LivBAofa2",
        "outputId": "916bfbce-b423-4e9b-8b1b-8fb13a5421f9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' externalToEVA', '�', '\\x07', '\\x1c', '�', '�', '\\x19', '龍�', '�', '\\x0b', '\\x14', '�', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', ' In', ' externalTo', ' gives', ' an', 'Although', ' 258', ' 1978', '\\x04', ' TheNitrome', '\\x1d', 'quickShip', '�', '\\x16', '\\r', '\\x15', '\\x02', ' in', '\\x01', '\\x17', 'In', '\\x12', '\\x18', 'izing', ' 15', '\\x0f', '\\x13', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', 'embedreportprint', '\\x1e', '\\x1f', 'laughs', '\\x1b', ' fork', ' RandomRedditor', 'nces', 'ifying', '39']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tev0cDbkdbxO"
      },
      "outputs": [],
      "source": [
        "ix = tokenizer.encode(\"\")\n",
        "# list of 'vocab indices'\n",
        "print(ix)\n",
        "print([tokenizer.decode(i) for i in ix])\n",
        "# prints reconstruction of input string\n",
        "print(len(ix))\n",
        "# prints number of tokens\n",
        "output_len=2\n",
        "model_out = model.generate(torch.tensor(ix).unsqueeze(0).to(device), max_length = output_len + len(ix))\n",
        "print(tokenizer.decode(model_out[0]))\n",
        "# pushes input string throught GPT2 (or whichever model) iteratively producing output_len number of tokens, then prints input + output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiments = [{'base_input': False, \n",
        "                'plt_loss': False, \n",
        "                'verbose': 1, \n",
        "                'epochs': 1000, \n",
        "                'lr_decay': False, \n",
        "                'return_early': False, \n",
        "                'lr': 0.1, \n",
        "                'batch_size': 20, \n",
        "                'target_output': ' a lot of data', \n",
        "                'output_len': 4, \n",
        "                'input_len': 3, \n",
        "                'w_freq': 20, \n",
        "                'dist_reg': 1, \n",
        "                'perp_reg': 0, \n",
        "                'loss_type': 'log_prob_loss',\n",
        "                'note':''}\n",
        "                ]\n",
        "\n",
        "\n",
        "experiment_log = {}"
      ],
      "metadata": {
        "id": "y8NBHj-uOnDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for e in experiments:\n",
        "    tick = time()\n",
        "    results = optimise_input(**e)\n",
        "    tock = time()\n",
        "    rt = tock - tick\n",
        "    results.update({'runtime':rt})\n",
        "    \n",
        "    with open(\"backwards_results.json\",\"a\") as f:\n",
        "        f.write(json.dumps({tick:results}))\n"
      ],
      "metadata": {
        "id": "81r4gT4m2NQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a13539aedc89401c8dc95605ff95c5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08bf66ccf8c74c6ab7101385415c7b41",
              "IPY_MODEL_23d9ec6919554b04b97d8ae95eadf7ec",
              "IPY_MODEL_ee6da364c2fd4f119a10ac77a88ca011"
            ],
            "layout": "IPY_MODEL_15997e2e26c04fb387f22b7a05669efa"
          }
        },
        "08bf66ccf8c74c6ab7101385415c7b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba2ee9137ad4737ae5ec69d90e9554c",
            "placeholder": "​",
            "style": "IPY_MODEL_c62c9d74e387404cbc9e3d9a3b0ccc76",
            "value": "Downloading: 100%"
          }
        },
        "23d9ec6919554b04b97d8ae95eadf7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5c3dd3dec744738a10f3ae84c42df2f",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4e37d9b6bf2499481a5ac9d74ef4a61",
            "value": 1042301
          }
        },
        "ee6da364c2fd4f119a10ac77a88ca011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec76f2ac7a384a33860b91b9b2024770",
            "placeholder": "​",
            "style": "IPY_MODEL_dfc992bded464758b040a6595e2c38a1",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.19MB/s]"
          }
        },
        "15997e2e26c04fb387f22b7a05669efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba2ee9137ad4737ae5ec69d90e9554c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62c9d74e387404cbc9e3d9a3b0ccc76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5c3dd3dec744738a10f3ae84c42df2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e37d9b6bf2499481a5ac9d74ef4a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec76f2ac7a384a33860b91b9b2024770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfc992bded464758b040a6595e2c38a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac6c91243664d2b8aada33b858edcd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e070ab3366e648999076851f1680725b",
              "IPY_MODEL_ecc6e036195f4178bfd6c8a2f84a92e5",
              "IPY_MODEL_99c978a555224bab98fc4602a137abd6"
            ],
            "layout": "IPY_MODEL_b4c96f32ac654f6fab35617265b8f9c4"
          }
        },
        "e070ab3366e648999076851f1680725b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f6b33ca4fa483d86cec40d0204e08e",
            "placeholder": "​",
            "style": "IPY_MODEL_b6370c98eaff4c409c3978a6ce82dc41",
            "value": "Downloading: 100%"
          }
        },
        "ecc6e036195f4178bfd6c8a2f84a92e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4947ff193a4c406ca7421cba5970e1d3",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_035f221ec4a643b4b8966b0ae22bdab1",
            "value": 456318
          }
        },
        "99c978a555224bab98fc4602a137abd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b4bf7b3659465a8354887e0500dc44",
            "placeholder": "​",
            "style": "IPY_MODEL_11f90c89e6604b5da2b318c9b5783724",
            "value": " 456k/456k [00:00&lt;00:00, 669kB/s]"
          }
        },
        "b4c96f32ac654f6fab35617265b8f9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69f6b33ca4fa483d86cec40d0204e08e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6370c98eaff4c409c3978a6ce82dc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4947ff193a4c406ca7421cba5970e1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035f221ec4a643b4b8966b0ae22bdab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9b4bf7b3659465a8354887e0500dc44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f90c89e6604b5da2b318c9b5783724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f893f2bece43415899c86d91fe0b00b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc96688312a740568f44c7ceeea3312e",
              "IPY_MODEL_ff67053d0dcd4ab8a0cd39928d6d112e",
              "IPY_MODEL_92e7ed3e02df4281a7b3de1d70ab4d9a"
            ],
            "layout": "IPY_MODEL_bbf523abfa3f492dbb5214ba94e36976"
          }
        },
        "bc96688312a740568f44c7ceeea3312e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc08c23b1304fdba26d6a25aaabb9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_afb014b62cbe4565840b9f8748cd32fe",
            "value": "Downloading: 100%"
          }
        },
        "ff67053d0dcd4ab8a0cd39928d6d112e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1198557a758f422d8a02a708f612648a",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93eec23f1b8d4f99930d45bdb8102690",
            "value": 665
          }
        },
        "92e7ed3e02df4281a7b3de1d70ab4d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaaa5f07ffe2433084f6c4a4079aa54d",
            "placeholder": "​",
            "style": "IPY_MODEL_fd003c97c2d740ce8cd6c1eb88c6e957",
            "value": " 665/665 [00:00&lt;00:00, 48.9kB/s]"
          }
        },
        "bbf523abfa3f492dbb5214ba94e36976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc08c23b1304fdba26d6a25aaabb9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb014b62cbe4565840b9f8748cd32fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1198557a758f422d8a02a708f612648a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93eec23f1b8d4f99930d45bdb8102690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaaa5f07ffe2433084f6c4a4079aa54d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd003c97c2d740ce8cd6c1eb88c6e957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b53c8f460d34173bead54f41c97326a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a886d06281a4d6bba1f756aa98f6f8b",
              "IPY_MODEL_9865bd1bb2b0438484b3cb6e2a592ddd",
              "IPY_MODEL_8de662f8f14a4920bee9322bf8cc9f94"
            ],
            "layout": "IPY_MODEL_c6381b0a8ed348c685e884fbbe9dfc9b"
          }
        },
        "9a886d06281a4d6bba1f756aa98f6f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47803c89c20c4f76a11988a963d541a9",
            "placeholder": "​",
            "style": "IPY_MODEL_72f06c169cd0485980cd2b66ecae94ab",
            "value": "Downloading: 100%"
          }
        },
        "9865bd1bb2b0438484b3cb6e2a592ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed96e266a52f46f0a8bf68bce6abd491",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9c8382ab1e24028b7c53a7f15a5deb1",
            "value": 548118077
          }
        },
        "8de662f8f14a4920bee9322bf8cc9f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5539bc849e9946f6aad5ac03c6be3ed3",
            "placeholder": "​",
            "style": "IPY_MODEL_7e404ba8a0414f52bc062d026e996ea4",
            "value": " 548M/548M [00:25&lt;00:00, 22.6MB/s]"
          }
        },
        "c6381b0a8ed348c685e884fbbe9dfc9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47803c89c20c4f76a11988a963d541a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72f06c169cd0485980cd2b66ecae94ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed96e266a52f46f0a8bf68bce6abd491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9c8382ab1e24028b7c53a7f15a5deb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5539bc849e9946f6aad5ac03c6be3ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e404ba8a0414f52bc062d026e996ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}